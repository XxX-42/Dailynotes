import os
import re
import random
import string
import unicodedata
import datetime
import threading
import time
from typing import Dict, List, Optional, Any, Set
from config import Config
from utils import Logger, FileUtils

class SyncCore:
    def __init__(self, state_manager):
        self.sm = state_manager
        self.project_map = {}
        self.project_path_map = {}
        self.file_path_map = {}

    def trigger_delayed_verification(self, filepath, delay=10):
        def _job():
            time.sleep(delay)
            # ä½¿ç”¨ FileUtils è¯»å–å†…å®¹
            content = FileUtils.read_file(filepath) or []
            # ä½¿ç”¨ç‹¬ç‰¹çš„æ ‡é¢˜è®°å½•æ—¥å¿—
            Logger.debug_block(f"VERIFICATION (T+{delay}s) Snapshot: {os.path.basename(filepath)}", content)
        
        t = threading.Thread(target=_job, daemon=True)
        t.start()

    def generate_block_id(self):
        return '^' + ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))

    def parse_yaml_tags(self, lines):
        tags = []
        if not lines or lines[0].strip() != '---': return []
        in_yaml = False
        for i, line in enumerate(lines):
            if i == 0: in_yaml = True; continue
            if line.strip() == '---': break
            if in_yaml and ('tags:' in line or 'main' in line):
                if re.search(r'\bmain\b', line): tags.append('main')
        return tags

    def clean_task_text(self, line, block_id=None, context_name=None):
        # [ä¿®å¤] ä¿å®ˆæ¸…ç†ï¼šä»…é’ˆå¯¹ç‰¹å®šæ¨¡å¼ï¼Œé¿å…è¿‡äºæ¿€è¿›çš„å…¨è¡Œæ­£åˆ™æ›¿æ¢
        
        # 1. ç§»é™¤æŒ‡å‘è‡ªèº«çš„å†…éƒ¨é“¾æ¥ (ä¾‹å¦‚ [[filename|...]])
        if context_name:
             # ä½¿ç”¨ç‰¹å®šæ¨¡å¼ä»…åŒ¹é… [[context_name|...]] æˆ– [[context_name]]
             # æ³¨æ„ä¸è¦åŒ¹é…é€šç”¨çš„ [[...]]
             line = re.sub(rf'\[\[{re.escape(context_name)}(?:\|.*?)?\]\]', '', line)

        # 2. çŠ¶æ€æ ‡è®°ï¼šç§»é™¤ "- [ ] " å‰ç¼€
        line = re.sub(r'^[\s>]*-\s*\[.\]\s?', '', line)
        
        # 3. å— IDï¼šç§»é™¤æœ«å°¾çš„ä¸¥æ ¼ ID
        # (å·²ä¸¥æ ¼ï¼šç©ºæ ¼ + ^ + 6-7 ä¸ªå­—æ¯æ•°å­— + ç»“å°¾)
        clean_text = line
        if block_id:
             # ä»…åœ¨ä¸¥æ ¼ç²¾ç¡®åŒ¹é…æ—¶ç§»é™¤
             # ä½¿ç”¨ re.split å¯èƒ½æ›´å¥½ï¼Œæˆ–è€…æ˜¯ç®€å•çš„å­—ç¬¦ä¸²æ›¿æ¢ï¼Ÿ
             # æ­£åˆ™è¡¨è¾¾å¼å¯¹äºè¾¹ç•Œå¤„ç†æ›´å®‰å…¨
             clean_text = re.sub(rf'(?<=\s)\^{re.escape(block_id)}\s*$', '', clean_text)
        
        # 4. ç§»é™¤æ—¥æœŸ/è¿æ¥ç¬¦ (é’ˆå¯¹æ€§)
        clean_text = re.sub(r'ğŸ“…\s*\d{4}-\d{2}-\d{2}', '', clean_text)
        clean_text = re.sub(r'âœ…\s*\d{4}-\d{2}-\d{2}', '', clean_text)
        clean_text = re.sub(r'\(connect::.*?\)', '', clean_text)
        clean_text = re.sub(r'\[\[[^\]]*?\|[â®ğŸ“…]\]\]', '', clean_text) # ç¬¦å·é“¾æ¥
        clean_text = re.sub(r'\[\[\d{4}-\d{2}-\d{2}\]\]', '', clean_text) # æ—¥æœŸé“¾æ¥

        return clean_text.strip()

    def normalize_block_content(self, block_lines):
        normalized = []
        for line in block_lines:
            clean = re.sub(r'^[\s>]+', '', line).strip()
            # [ä¿®å¤] å¹½çµå­å¼¹è¿‡æ»¤å™¨ï¼šå¿½ç•¥ç©ºè¡Œæˆ–ä»…æœ‰å­å¼¹ç‚¹çš„è¡Œ
            if not clean or clean in ['-', '- ']: continue
            normalized.append(clean)
        # [ä¿®å¤] ç‰©ç†é˜²ç²˜è¿ï¼šä½¿ç”¨æ¢è¡Œç¬¦è¿æ¥ï¼Œé˜²æ­¢ # å·è¢«åå™¬
        # åŸé€»è¾‘ï¼šreturn "".join(normalized) -> å±é™©ï¼
        return "\n".join(normalized) + "\n"

    def extract_routing_target(self, line):
        clean = re.sub(r'\[\[[^\]]*?\#\^[a-zA-Z0-9]{6,}\|[âš“\*ğŸ”—â®ğŸ“…]\]\]', '', line)
        matches = re.findall(r'\[\[(.*?)\]\]', clean)
        for match in matches:
            pot = match.split('|')[0]
            pot = unicodedata.normalize('NFC', pot)
            if pot in self.file_path_map: return self.file_path_map[pot]
        return None

    def capture_block(self, lines, start_idx):
        if start_idx >= len(lines): return [], 0
        
        def get_indent(s):
            # è®¡ç®—ç¼©è¿›ï¼Œå¿½ç•¥ '>' å‰ç¼€
            # 1. å»é™¤ '>' å’Œå¯é€‰ç©ºæ ¼
            no_quote = re.sub(r'^>\s?', '', s)
            # 2. è®¡ç®—ç›¸å¯¹äºå¹²å‡€å­—ç¬¦ä¸²çš„å‰å¯¼ç©ºæ ¼
            return len(no_quote) - len(no_quote.lstrip())

        base_indent = get_indent(lines[start_idx])
        block = [lines[start_idx]]
        consumed = 1
        j = start_idx + 1
        while j < len(lines):
            nl = lines[j]
            
            # è§„åˆ™ A: åŸå§‹è¡Œæ£€æŸ¥ (å¦‚æœåŸå§‹è¡Œå°±æ˜¯ # Headerï¼Œå³ä½¿è¢«å¼•ç”¨é€»è¾‘å¤„ç†å‰ä¹Ÿåº”ä¸­æ­¢)
            if nl.lstrip().startswith('#'): break

            # è§„åˆ™ B: å‰¥ç¦»å¼•ç”¨ç¬¦å· (> å’Œç©ºç™½) åçš„æ£€æŸ¥
            # å¿…é¡»å…ˆå‰¥ç¦»å¼•ç”¨ç¬¦å·ï¼ˆ>ï¼‰å’Œç©ºç™½ï¼Œä»¥æ•æ‰åƒ ">   # Header" è¿™æ ·çš„æƒ…å†µ
            stripped_check = re.sub(r'^[>\s]+', '', nl)
            
            # è§„åˆ™ B.1: æ·±åº¦æ ‡é¢˜æ£€æŸ¥
            if stripped_check.startswith('#'): break
            
            # è§„åˆ™ C: åˆ†éš”ç¬¦æ£€æŸ¥
            if stripped_check.startswith('---'): break

            # ç©ºè¡Œæ˜¯å—çš„ä¸€éƒ¨åˆ†å—ï¼Ÿæ˜¯çš„ï¼Œå¦‚æœæœ‰ç¼©è¿›æˆ–åœ¨å—é€»è¾‘å†…éƒ¨ã€‚
            # ä½†è¿™é‡Œæˆ‘ä»¬åªæ£€æŸ¥å®ƒæ˜¯å¦â€œç›¸å¯¹äºåŸºå‡†ç¼©è¿›â€ã€‚
            # å¯¹äºç©ºè¡Œï¼Œget_indent å¯èƒ½æ˜¯ 0ã€‚
            if nl.strip() == "": 
                block.append(nl); consumed += 1; j += 1; continue
            
            if get_indent(nl) > base_indent:
                block.append(nl); consumed += 1; j += 1
            else:
                break
        return block, consumed
    def normalize_raw_tasks(self, lines, filename_stem):
        """
        è‡ªåŠ¨æ³¨å†Œï¼šæ£€æµ‹æ²¡æœ‰ ID çš„åŸå§‹ä»»åŠ¡ '> - [ ]' å¹¶è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼ã€‚
        """
        if not lines or not filename_stem: return lines
        
        new_lines = []
        today_str = datetime.date.today().strftime("%Y-%m-%d")
        raw_pattern = re.compile(r'^(>\s*-\s*\[\s*\])(.*)$')
        id_pattern = re.compile(r'\^[a-z0-9]{6}\s*$')

        def generate_id():
            return ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))

        for line in lines:
            match = raw_pattern.match(line)
            if match:
                prefix = match.group(1) 
                text_body = match.group(2).strip()
                
                if not id_pattern.search(text_body):
                    new_id = generate_id()
                    formatted_body = f"[[{filename_stem}#^{new_id}|â®]] [[{today_str}]]"
                    if text_body:
                        formatted_body += f" {text_body}"
                    new_lines.append(f"{prefix} {formatted_body} ^{new_id}")
                else:
                    new_lines.append(line)
            else:
                new_lines.append(line)
        return new_lines

    def inject_into_callout(self, file_lines, block_lines, filename_stem=None):
        """
        é‡å†™ï¼ŒåŒ…å«æ·±åº¦è¿½è¸ªæ—¥å¿—å’Œç¡¬é™åˆ¶å°¾éƒ¨é€»è¾‘ã€‚
        """
        # --- 1. æå– YAML å¹¶åˆ†å‰²æ­£æ–‡ ---
        yaml_lines = []
        body_lines = []
        if file_lines and file_lines[0].strip() == '---':
            yaml_lines.append(file_lines[0])
            for i in range(1, len(file_lines)):
                yaml_lines.append(file_lines[i])
                if file_lines[i].strip() == '---':
                    body_lines = file_lines[i+1:]
                    break
            else:
                yaml_lines = []
                body_lines = file_lines
        else:
            body_lines = file_lines

        # --- 2. æ”¶å‰²é˜¶æ®µ ---
        harvested_tasks = []
        clean_body = []
        default_header = "> [!note]- Tasks"
        captured_header = None
        
        # [è¡¥ä¸] é²æ£’çš„æ ‡é¢˜æ­£åˆ™ (æ”¯æŒ > [!note], > [!note]-, > [!note]+)
        TASK_HEADER_PATTERN = re.compile(r"^>\s*\[!note\]([-+]?)\s+Tasks", re.IGNORECASE)
        
        i = 0
        while i < len(body_lines):
            line = body_lines[i]
            stripped = line.strip()
            
            # æƒ…å†µ A: ç°æœ‰ Callout (æ­£åˆ™æ£€æµ‹)
            match = TASK_HEADER_PATTERN.match(stripped)
            if match:
                if not captured_header:
                     captured_header = line.strip() # ä¿ç•™æ‰¾åˆ°çš„ç¬¬ä¸€ä¸ªæ ‡é¢˜
                i += 1
                while i < len(body_lines):
                    cl = body_lines[i]
                    if cl.strip().startswith('>'):
                         harvested_tasks.append(cl)
                         i += 1
                    elif cl.strip() == '':
                         # å®½æ¾æ ¼å¼å¯å‘å¼
                         if i + 1 < len(body_lines) and body_lines[i+1].strip().startswith('>'):
                             harvested_tasks.append(cl)
                             i += 1
                         else:
                             break 
                    else:
                         break 
                continue

            # æƒ…å†µ B: å­¤ç«‹ä»»åŠ¡
            if re.match(r'^[\s]*-\s*\[.\]', line):
                 has_id = re.search(r'\^[a-zA-Z0-9]{6,}\s*$', line)
                 if has_id:
                     block, consumed = self.capture_block(body_lines, i)
                     harvested_tasks.extend(block)
                     i += consumed
                     continue
            
            # æƒ…å†µ C: æ™®é€šæ–‡æœ¬
            clean_body.append(line)
            i += 1

        # æ—¥å¿— 1: å·²æ”¶å‰²
        if block_lines or harvested_tasks:
             Logger.debug(f"DeepTrace: Harvested {len(harvested_tasks)} existing, {len(block_lines)} new.")

        # --- 3. æ ‡å‡†åŒ–ä¸å»é‡ ---
        candidates = harvested_tasks + block_lines
        processed_candidates = []
        seen_ids = set()
        
        for line in candidates:
            clean_l = re.sub(r'^>\s?', '', line)
            # æ ‡å‡†åŒ–ç©ºè¡Œ
            if clean_l.strip() == '':
                processed_candidates.append("") 
                continue
            
            if '\t' in clean_l:
                clean_l = clean_l.replace('\t', '    ') 

            id_match = re.search(r'\^([a-zA-Z0-9]{6,})\s*$', clean_l)
            if id_match:
                bid = id_match.group(1)
                if bid in seen_ids:
                    continue 
                seen_ids.add(bid)
            
            # [ä¿®å¤] ä¿ç•™å°¾éšç©ºæ ¼ (ä¾‹å¦‚ "- ") ä¾›ç”¨æˆ·å…‰æ ‡ä½¿ç”¨
            processed_candidates.append(clean_l.rstrip('\n\r')) 
        
        # æ—¥å¿— 2: å·²å¤„ç†
        # Logger.debug(f"DeepTrace: {len(processed_candidates)} candidates after norm.")

        # --- 4. ä¸Šä¸‹æ–‡æ„ŸçŸ¥å®‰å…¨å‹ç¼© ---
        final_task_lines = []
        
        def is_list_item(s):
            if not s: return False
            return re.match(r'^\s*([-\*]|\d+\.)\s', s) is not None

        last_content_line = "HEADER"

        for j, curr in enumerate(processed_candidates):
            if curr == "":
                next_l = processed_candidates[j+1] if j < len(processed_candidates) - 1 else None
                
                # 1. å¡Œç¼©é‡å¤çš„ç©ºè¡Œ
                if next_l == "": continue 

                # 2. åˆ—è¡¨é¡¹é—´éš™ -> ç§»é™¤
                prev_is_item = (last_content_line == "HEADER") or is_list_item(last_content_line)
                next_is_item = is_list_item(next_l)
                
                if prev_is_item and next_is_item:
                    continue # SKIP
                
                # 3. ä¿ç•™æ®µè½ä¸­æ–­ (å¸¦ç©ºæ ¼)
                # [ä¿®å¤] è¾“å‡º "> \n" è€Œä¸æ˜¯ ">\n" ä»¥ä¿ç•™ "ç­‰å¾…è¾“å…¥" çš„ç©ºæ ¼ã€‚
                # è¿™é˜²æ­¢äº† "ç©ºæ ¼åˆ†éš”" é—®é¢˜å’Œ Obsidian çš„å¯¹æŠ—ã€‚
                final_task_lines.append("> \n")
            else:
                final_task_lines.append(f"> {curr}\n")
                last_content_line = curr

        # --- 5. æ¿€è¿›æ¸…ç†å™¨ (é›¶å°¾éƒ¨) ---
        # è‡ªåŠ¨æ³¨å†Œï¼šå°†åŸå§‹ä»»åŠ¡è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        if filename_stem:
            final_task_lines = self.normalize_raw_tasks(final_task_lines, filename_stem)
            
        # Apply Aggressive Callout Cleaner
        final_task_lines = self.aggressive_callout_clean(final_task_lines)

        # æ—¥å¿— 3: æœ€ç»ˆç»“æœ
        if final_task_lines:
             Logger.debug(f"DeepTrace: Final Block has {len(final_task_lines)} lines.")

        # --- 6. é‡å»º ---
        final_header = captured_header if captured_header else default_header
        
        new_block = []
        if final_task_lines:
            new_block.append(f"{final_header}\n")
            new_block.extend(final_task_lines)
            # [CRITICAL FIX] ç§»é™¤äº†é‡å¤çš„ extend è°ƒç”¨
            new_block.append("> \n") # [ä¿®å¤] å¼ºåˆ¶éš”ç¦»ï¼šä»»åŠ¡å—åè¿½åŠ ç©ºå¼•ç”¨è¡Œ
            new_block.append("\n")   # [ä¿®å¤] ç‰©ç†éš”ç¦»ï¼šè¿½åŠ ç‰©ç†ç©ºè¡Œä»¥åŒºéš”åç»­æ ‡é¢˜

        return yaml_lines + new_block + clean_body

    def aggressive_callout_clean(self, lines):
        """
        [çƒ­ä¿®å¤] æ”¾å®½çš„ Callout æ¸…ç†å™¨ã€‚
        ä»…å½“è¿ç»­ç©ºè¡Œè¶…è¿‡ 2 è¡Œæ—¶ç§»é™¤ã€‚
        ä¿ç•™ç‰¹æ®Šå­—ç¬¦å¦‚ '---' å’Œåˆ—è¡¨æ ‡è®° '-'ã€‚
        è®°å½•åˆ é™¤æ“ä½œä»¥ä¾›è°ƒè¯•ã€‚
        """
        if not lines: return []
        
        cleaned_lines = []
        empty_count = 0
        
        # "Callout ç©ºè¡Œ" çš„æ­£åˆ™ï¼š> åè·Ÿå¯é€‰ç©ºç™½
        # ä¸åŒ¹é… > - (å­å¼¹ç‚¹) æˆ– > text
        empty_pattern = re.compile(r'^\s*>\s*$')
        
        for i, line in enumerate(lines):
            is_empty = bool(empty_pattern.fullmatch(line))
            
            # ç‰¹æ®Šå®‰å…¨æªæ–½ï¼šå¦‚æœè¡ŒåŒ…å« '---' æˆ– '-'ï¼Œè§†ä¸ºå†…å®¹
            # '-' ä¿æŠ¤åˆ—è¡¨çš„è¾“å…¥æµ (ä¾‹å¦‚ "> -")
            if '---' in line or '-' in line:
                is_empty = False
            
            if is_empty:
                empty_count += 1
                if empty_count > 2:
                    # è¿‡å¤šç©ºè¡Œ -> è·³è¿‡/åˆ é™¤
                    Logger.debug(f"[CLEAN] Removing excess callout line {i+1}: {repr(line)}")
                    continue 
                else:
                    cleaned_lines.append(line)
            else:
                # å‘ç°å†…å®¹ -> é‡ç½®è®¡æ•°å™¨
                empty_count = 0
                cleaned_lines.append(line)
        
        return cleaned_lines

    def aggressive_daily_clean(self, lines: list) -> list:
        """
        [çƒ­ä¿®å¤] æ”¾å®½çš„æ¯æ—¥æ¸…ç†å™¨ã€‚
        ä»…å½“æ­£æ–‡ä¸­è¿ç»­ç©ºè¡Œè¶…è¿‡ 2 è¡Œæ—¶ç§»é™¤ã€‚
        ä¿ç•™ç‰¹æ®Šå­—ç¬¦å¦‚ '---'ã€‚
        è®°å½•åˆ é™¤æ“ä½œä»¥ä¾›è°ƒè¯•ã€‚
        """
        if not lines: return []

        # 1. è¯†åˆ« "é¡µè„š" ç´¢å¼•
        footer_idx = len(lines)
        for i, line in enumerate(lines):
            if line.strip().startswith('# Day planner') or line.strip().startswith('# Journey'):
                footer_idx = i
                break
        
        # 2. æå–é¡µè„šä¸Šæ–¹çš„å†…å®¹
        body = lines[:footer_idx]
        foot = lines[footer_idx:]
        
        # 3. æ¸…ç†æ­£æ–‡ï¼ˆç”±äºå†…éƒ¨å‚ç›´ç©ºç™½ï¼‰
        cleaned_body = []
        empty_count = 0
        empty_pattern = re.compile(r'^\s*$') # Matches pure blank lines
        
        for i, line in enumerate(body):
            is_empty = bool(empty_pattern.fullmatch(line))
            
            # å®‰å…¨ï¼šä¿æŠ¤ '---' å’Œå…¶ä»–ç»“æ„æ ‡è®°
            if '---' in line:
                is_empty = False
                
            if is_empty:
                empty_count += 1
                if empty_count > 2:
                    Logger.debug(f"[CLEAN] Removing excess daily line {i+1}: {repr(line)}")
                    continue
                else:
                    cleaned_body.append(line)
            else:
                empty_count = 0
                cleaned_body.append(line)
        
        # 4. é‡æ–°ç»„è£…
        return cleaned_body + foot

    def format_line(self, indent, status, text, dates, fname, bid, is_daily):
        # [ç‰¹æ€§] ä½¿ç”¨ TAB è¿›è¡Œç¼©è¿›
        # åŸºäºç¼©è¿›çº§åˆ«è®¡ç®—åˆ¶è¡¨ç¬¦æ•°é‡ï¼ˆå‡è®¾ 1 çº§ = 4 ä¸ªç©ºæ ¼æˆ– 1 ä¸ªåˆ¶è¡¨ç¬¦ï¼‰
        # å¦‚æœ 'indent' ä»…ä»…ä½œä¸ºç‰¹å®šå®½åº¦ä¼ å…¥ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦è°ƒæ•´ã€‚
        # ä½† 'indent' æ˜¯é€šè¿‡è®¡ç®—å­—ç¬¦æ•°çš„ get_indent() è®¡ç®—å¾—å‡ºçš„ã€‚
        # ç®€å•ä¿®å¤ï¼šå¦‚æœæˆ‘ä»¬æ›´æ”¹è¾“å…¥é€»è¾‘ï¼Œå°† 'indent' è§†ä¸ºåˆ¶è¡¨ç¬¦æ•°é‡ï¼Ÿ
        # ä¸ï¼Œ'indent' æ˜¯åŸå§‹æ•´æ•°ã€‚è®©æˆ‘ä»¬è½¬æ¢ï¼š4 ä¸ªç©ºæ ¼ -> 1 ä¸ªåˆ¶è¡¨ç¬¦ã€‚
        tab_count = indent // 4 
        indent_str = '\t' * tab_count
        
        if is_daily:
            link = f"[[{fname}#^{bid}|â®]]"
            time_match = re.match(r'^(\d{1,2}:\d{2}(?:\s*-\s*\d{1,2}:\d{2})?)', text)
            if time_match:
                time_part = time_match.group(1)
                rest_part = text[len(time_part):].strip()
                return f"{indent_str}- [{status}] {time_part} {link} {rest_part} ^{bid}\n"
            else:
                return f"{indent_str}- [{status}] {link} {text} ^{bid}\n"
        else:
            clean_text = re.sub(r'\d{1,2}:\d{2}\s*-\s*\d{1,2}:\d{2}', '', text)
            clean_text = re.sub(r'\d{1,2}:\d{2}', '', clean_text)
            clean_text = re.sub(rf'\[\[{re.escape(fname)}(\|.*?)?\]\]', '', clean_text)
            clean_text = clean_text.strip()

            creation_date = None
            # Extract simple date from dates string or complex link
            # dates string comes from scanner
            simple_match = re.search(r'\[\[(\d{4}-\d{2}-\d{2})\]\]', dates)
            if simple_match:
                creation_date = simple_match.group(1)
            else:
                 # Try finding from complex/emoji
                 m = re.search(r'(?:ğŸ“…|\|ğŸ“…\]\])\s*(\d{4}-\d{2}-\d{2})', dates)
                 if m: creation_date = m.group(1)
            
            if not creation_date:
                # Last resort fallback if date not found in dates string
                 m = re.search(r'(\d{4}-\d{2}-\d{2})', dates)
                 if m: creation_date = m.group(1)

            processed_dates = []
            done_date_match = re.search(r'âœ…\s*(\d{4}-\d{2}-\d{2})', dates)
            if done_date_match:
                processed_dates.append(f"âœ… {done_date_match.group(1)}")

            meta_str = " ".join(processed_dates)
            
            # Construct Links
            self_link = f"[[{fname}#^{bid}|â®]]"
            daily_link = f"[[{creation_date}]]" if creation_date else ""
            parts = [self_link]
            if daily_link: parts.append(daily_link)
            parts.append(clean_text)
            if meta_str: parts.append(meta_str)
            parts.append(f"^{bid}")
            
            content_str = " ".join(parts)
            return f"{indent_str}- [{status}] {content_str}\n"

    # [æ–°] ç»Ÿä¸€æ ¼å¼åŒ–çš„è¾…åŠ©å‡½æ•°
    def normalize_child_lines(self, raw_lines, parent_indent, as_quoted=False):
        children = []
        child_indent_lvl = (parent_indent // 4) + 1
        child_indent_str = '\t' * child_indent_lvl
        
        for line in raw_lines:
             # æ¸…ç†ï¼šç§»é™¤ > å’Œç©ºæ ¼
             content = re.sub(r'^[>\s]+', '', line).strip()
             
             # [ä¿®å¤] å¹½çµå­å¼¹è¿‡æ»¤å™¨ï¼šè·³è¿‡ç©ºè¡Œæˆ–çº¯çŸ­æ¨ªçº¿è¡Œ
             if not content or content in ['-', '- ']: continue

             # å¼ºåˆ¶å­å¼¹ç‚¹è¯­æ³•
             if content.startswith('-'):
                 if not content.startswith('- '):
                      final_content = "- " + content[1:].strip()
                 else:
                      final_content = content
             else:
                  final_content = f"- {content}"
             
             # è¾“å‡ºç»„è£…
             if as_quoted:
                 # æºæ–‡ä»¶ï¼šæ£€æŸ¥æ˜¯å¦ä¸ºç©ºä»¥é¿å…å°¾éšç©ºæ ¼é—®é¢˜ï¼Ÿ
                 # æ ‡å‡†ï¼š"> \t- content"
                 # ç‰¹æ®Šï¼š"> " ç”¨äºç©ºè¡Œï¼Ÿä¸ï¼Œå¯¹äºå­å¼¹ç‚¹è¡Œæˆ‘ä»¬ä½¿ç”¨è¯­æ³•ã€‚
                 children.append(f"> {child_indent_str}{final_content}\n")
             else:
                 # æ¯æ—¥ç¬”è®°ï¼šçº¯æ–‡æœ¬
                 children.append(f"{child_indent_str}{final_content}\n")
                 
        return children

    def reconstruct_daily_block(self, sd, target_date):
        fname = sd['fname']
        bid = sd['bid']
        status = sd['status']
        
        # 1. æ¸…ç†æ–‡æœ¬ï¼šç§»é™¤æ—¥æœŸé“¾æ¥
        text = re.sub(r'\[\[\d{4}-\d{2}-\d{2}\]\]', '', sd['pure']).strip()
        
        # 2. å¦‚æœç¼ºå¤±ï¼Œé‡æ–°æ³¨å…¥é¡¹ç›®é“¾æ¥
        link_tag = f"[[{fname}]]"
        if link_tag not in text:
            text = f"{link_tag} {text}"
            
        # 3. æ„å»ºçˆ¶è¡Œï¼ˆæ¯æ—¥æ ¼å¼ï¼‰
        # çˆ¶ç¼©è¿›ä½¿ç”¨åˆ¶è¡¨ç¬¦ï¼Ÿé€šå¸¸æ¯æ—¥ç¬”è®°æ˜¯é¡¶å±‚è¿˜æ˜¯ç¼©è¿›çš„ï¼Ÿ
        # é€šå¸¸æ˜¯é¡¶å±‚æˆ– format_line ç”Ÿæˆçš„ä»»ä½•å†…å®¹ã€‚
        # ç­‰ç­‰ï¼Œsd['indent'] æ˜¯æºç¼©è¿›ã€‚æ¯æ—¥ç¬”è®°ç¼©è¿›åº”è¯¥æ˜¯ç›¸å¯¹çš„å—ï¼Ÿ
        # å¦‚æœæºæ˜¯ç¼©è¿›çš„ï¼Œæ¯æ—¥ç¬”è®°æ„å‘³ç€æ‰å¹³åŒ–ï¼Ÿ
        # ä¸ï¼Œé€šå¸¸æ¯æ—¥ç¬”è®°èšåˆä»»åŠ¡ã€‚
        # ä½†è®©æˆ‘ä»¬åšæŒä½¿ç”¨çˆ¶çº§çš„ format_line é€»è¾‘ã€‚
        parent_line = self.format_line(sd['indent'], status, text, "", fname, bid, True)
        
        # 4. å¼ºåˆ¶å­é¡¹æ ¼å¼åŒ–ï¼ˆæš´åŠ›ï¼‰
        children = []
        raw_children = sd['raw'][1:]
        
        # è®¡ç®—å­ç¼©è¿›ï¼ˆä¸¥æ ¼æ¯”çˆ¶çº§ +1 çº§ï¼‰
        # å‡è®¾çˆ¶çº§åœ¨ sd['indent']
        # ç­‰ç­‰ï¼Œæ¯æ—¥ç¬”è®°ä¸­çš„ parent_line é€šå¸¸ä» 0 å¼€å§‹è¿˜æ˜¯ä¿ç•™ï¼Ÿ
        # å¦‚æœæˆ‘ä»¬ç›´æ¥å¯¹çˆ¶çº§ä½¿ç”¨ sd['indent']ï¼Œæˆ‘ä»¬ä¿ç•™å±‚çº§ã€‚
        # é‚£ä¹ˆå­é¡¹åœ¨çˆ¶çº§ +1 çº§ã€‚
        child_indent_str = '\t' * ((sd['indent'] // 4) + 1)
        
        for line in raw_children:
            # 4.1 ç§»é™¤ Callout å­—ç¬¦
            # æ­£åˆ™ï¼šç§»é™¤å¼€å¤´çš„ '>' å’Œå¯é€‰ç©ºæ ¼
            child_clean = re.sub(r'^>\s?', '', line)
            
            # 4.2 åˆ†æå†…å®¹
            stripped = child_clean.strip()
            
            # 4.3 å†…å®¹é‡æ„
            if not stripped or stripped == '-':
                # æƒ…å†µï¼šç©ºå­å¼¹ç‚¹
                final_content = "- "
            elif stripped.startswith('-'):
                # æ£€æŸ¥ç²˜è¿ï¼Œä¾‹å¦‚ "-Text"
                # å¦‚æœåŒ¹é… "-[ä»»ä½•å†…å®¹]"
                if len(stripped) > 1 and stripped[1] != ' ':
                     # å¼ºåˆ¶ç©ºæ ¼ï¼š"-Text" -> "- Text"
                     final_content = f"- {stripped[1:].strip()}"
                elif stripped == '- ':
                     final_content = "- "
                else:
                     # æ˜¯ "- Text" æˆ– "- [ ] Text"
                     # é‡å»ºä»¥ç¡®ä¿å®‰å…¨
                     # å»é™¤å‰å¯¼ "- " å¹¶é‡æ–°æ·»åŠ ï¼Ÿ
                     # å¦‚æœæ˜¯ "- "ï¼Œstripped[2:] å¯èƒ½æ˜¯ç©ºçš„
                     final_content = f"- {stripped[2:].strip()}"
            else:
                # æƒ…å†µï¼š"Text"ï¼ˆç¼ºå°‘å­å¼¹ç‚¹ï¼‰
                final_content = f"- {stripped}"
                
            # 4.4 ç¼©è¿›æ³¨å…¥
            # [å…³é”®] ç¡®ä¿ "- " ä¸­ä¿ç•™ç©ºæ ¼
            formatted_line = f"{child_indent_str}{final_content}"
            # åŒé‡æ£€æŸ¥ç©ºå­å¼¹ç‚¹çš„å°¾éšç©ºæ ¼
            if formatted_line.strip() == '-': 
                 # è¿™ä¸åº”è¯¥å‘ç”Ÿï¼Œç”±äºä¸Šé¢çš„é€»è¾‘
                 formatted_line += " "
            elif formatted_line.endswith('-'):
                 formatted_line += " "

            children.append(formatted_line)

        # [ä¼˜åŒ–å‹ç¼©]
        # å…è®¸æœ«å°¾æœ€å¤š 1 ä¸ªç©ºå­å¼¹ç‚¹
        if children:
            while children and children[-1].strip() == '-':
                children.pop()
            # If we popped everything or want to leave one breathing room?
            # ç”¨æˆ·ä¹‹å‰æƒ³è¦ "æœ€å¤š 1 ä¸ª"ã€‚
            # å¦‚æœæˆ‘å¼¹å‡ºæ‰€æœ‰ï¼Œé‚£å°±æ˜¯ 0 ä¸ªã€‚
            # å¦‚æœæœ‰æ•ˆåˆ—è¡¨ä¸ä¸ºç©ºï¼Œæˆ‘ä»¬è¦åŠ å›ä¸€ä¸ªå—ï¼Ÿ
            # è¿˜æ˜¯ä¿æŒç´§å‡‘ã€‚
            # "æš´åŠ›" é€šå¸¸æ„å‘³ç€ä¸¥æ ¼ã€‚
            # è®©æˆ‘ä»¬çœ‹çœ‹ä¹‹å‰çš„è¡Œä¸ºï¼š"ç¡®ä¿æœ€å¤š 1 ä¸ªç©ºå­å¼¹ç‚¹"ã€‚
            # å¦‚æœæˆ‘ç§»é™¤äº†ä»»ä½•å­å¼¹ç‚¹ï¼Œæˆ‘ä¼šè¿½åŠ ä¸€ä¸ªç©ºå­å¼¹ç‚¹å—ï¼Ÿä¸ã€‚
            # åªæ˜¯ç®€å•åœ°ï¼šç§»é™¤å°¾éšå­å¼¹ç‚¹ã€‚
            # å¦‚æœç”¨æˆ·æƒ³è¦ï¼Œä»–ä»¬å¯ä»¥è¾“å…¥ã€‚
            # å¦‚æœæˆ‘è‡ªåŠ¨æ·»åŠ ï¼Œé‚£å°±æ˜¯ "å¹½çµå­å¼¹"ã€‚
            # è®©æˆ‘ä»¬æ¸…ç†æ‰€æœ‰å°¾éšç©ºå­å¼¹ç‚¹ã€‚
            pass

        return [parent_line] + children

    def cleanup_empty_callouts(self, lines):
        """å¦‚æœ '> [!note]- Tasks' (æˆ–å˜ä½“) å—ä¸­ä¸åŒ…å«ä»»åŠ¡å¤é€‰æ¡†ï¼Œåˆ™å°†å…¶ç§»é™¤ã€‚"""
        if lines is None: return [] # å«è¯­å¥
        
        output = []
        # é²æ£’çš„æ ‡é¢˜æ­£åˆ™ (æ”¯æŒ > [!note], > [!note]-, > [!note]+)
        TASK_HEADER_PATTERN = re.compile(r"^>\s*\[!note\]([-+]?)\s+Tasks", re.IGNORECASE)
        
        in_callout = False
        callout_buffer = []
        has_task = False

        for line in lines:
            if TASK_HEADER_PATTERN.match(line.strip()):
                # å¦‚æœéœ€è¦ï¼Œåˆ·æ–°ä¹‹å‰çš„ï¼ˆä¸åº”å¤„ç†åµŒå¥—ï¼‰
                if in_callout:
                     if has_task: output.extend(callout_buffer)
                in_callout = True
                callout_buffer = [line]
                has_task = False
            elif in_callout:
                if line.strip().startswith('>') or line.strip() == '':
                    callout_buffer.append(line)
                    if re.search(r'-\s*\[.\]', line):
                        has_task = True
                else:
                    # Callout ç»“æŸ
                    if has_task: output.extend(callout_buffer)
                    in_callout = False
                    callout_buffer = []
                    output.append(line)
            else:
                output.append(line)
        
        if in_callout and has_task:
            output.extend(callout_buffer)

        return output

    def ensure_structure(self, lines):
        has_dp = any(l.strip() == "# Day planner" for l in lines)
        j_idx = -1
        try:
            j_idx = next(i for i, l in enumerate(lines) if l.strip() == "# Journey")
        except StopIteration:
            pass

        if not has_dp:
            if j_idx != -1:
                lines.insert(j_idx, "# Day planner\n\n")
            else:
                lines.insert(0, "# Day planner\n\n")
                lines.append("\n# Journey\n")

        if has_dp and j_idx == -1:
            lines.append("\n# Journey\n")

        return lines

    def cleanup_empty_headers(self, lines, date_tag):
        # [ä¿®å¤] é˜‰å‰²æ ‡é¢˜åå™¬ï¼š
        # ä»…ç¡®ä¿ç»“æ„ï¼Œä¸è¦åˆ é™¤ç©ºæ ‡é¢˜ã€‚
        lines = self.ensure_structure(lines)
        return lines, False

    def scan_projects(self):
        self.project_map = {}
        self.project_path_map = {}
        self.file_path_map = {}
        for root, dirs, files in os.walk(Config.ROOT_DIR):
            dirs[:] = [d for d in dirs if not FileUtils.is_excluded(os.path.join(root, d))]
            if FileUtils.is_excluded(root): continue
            main_files = []
            for f in files:
                if f.endswith('.md'):
                    path = os.path.join(root, f)
                    f_name = unicodedata.normalize('NFC', os.path.splitext(f)[0])
                    self.file_path_map[f_name] = path
                    if 'main' in self.parse_yaml_tags(FileUtils.read_file(path) or []):
                        main_files.append(f)
            if len(main_files) == 1:
                p_name = unicodedata.normalize('NFC', os.path.splitext(main_files[0])[0])
                self.project_map[root] = p_name
                self.project_path_map[p_name] = os.path.join(root, main_files[0])

    def organize_orphans(self, filepath, date_tag):
        lines = FileUtils.read_file(filepath)
        if not lines: return False

        lines = self.ensure_structure(lines)

        tasks_to_move = []
        ctx = "ROOT"
        i = 0
        while i < len(lines):
            l = lines[i].strip()
            if l.startswith('# '):
                ctx = 'JOURNEY' if l == '# Journey' else ('PLANNER' if l == '# Day planner' else 'OTHER')
                i += 1;
                continue
            if l.startswith('## [[') and l.endswith(']]'): ctx = 'PROJECT'; i += 1; continue

            if re.match(r'^[\s>]*-\s*\[.\]', lines[i]):
                if ctx in ['JOURNEY', 'PLANNER']:
                    target = self.extract_routing_target(lines[i])
                    if target:
                        p_root = os.path.dirname(target)
                        p_name = None
                        curr = p_root
                        while curr.startswith(Config.ROOT_DIR):
                            if curr in self.project_map: p_name = self.project_map[curr]; break
                            parent = os.path.dirname(curr)
                            if parent == curr: break
                            curr = parent
                        if p_name:
                            content, length = self.capture_block(lines, i)
                            tasks_to_move.append({'idx': i, 'len': length, 'proj': p_name, 'raw': content})
                            i += length;
                            continue
            i += 1

        if not tasks_to_move: return False

        tasks_to_move.sort(key=lambda x: x['idx'], reverse=True)
        for t in tasks_to_move: del lines[t['idx']:t['idx'] + t['len']]

        grouped = {}
        for t in tasks_to_move:
            if t['proj'] not in grouped: grouped[t['proj']] = []
            grouped[t['proj']].extend(t['raw'])

        try:
            j_idx = next(i for i, l in enumerate(lines) if l.strip() == "# Journey")
        except:
            j_idx = len(lines)
        ins_pt = len(lines)
        for i in range(j_idx + 1, len(lines)):
            if lines[i].startswith('# '): ins_pt = i; break

        offset = 0
        for proj, blocks in grouped.items():
            header = f"## [[{proj}]]"
            h_idx = -1
            for k in range(j_idx, ins_pt + offset):
                if lines[k].strip() == header: h_idx = k; break

            if blocks and not blocks[-1].endswith('\n'): blocks[-1] += '\n'

            if h_idx != -1:
                sub_ins = ins_pt + offset
                for k in range(h_idx + 1, ins_pt + offset):
                    if lines[k].startswith('#'): sub_ins = k; break
                lines[sub_ins:sub_ins] = blocks
                offset += len(blocks)
            else:
                chunk = [f"\n{header}\n"] + blocks
                lines[ins_pt + offset:ins_pt + offset] = chunk
                offset += len(chunk)

        Logger.info(f"å½’æ¡£ {len(tasks_to_move)} ä¸ªæµæµªä»»åŠ¡", date_tag)
        return FileUtils.write_file(filepath, lines)

    def scan_all_source_tasks(self) -> Dict[str, Dict]:
        self.scan_projects()
        source_data_by_date = {}
        today_str = datetime.date.today().strftime('%Y-%m-%d')

        for root, dirs, files in os.walk(Config.ROOT_DIR):
            dirs[:] = [d for d in dirs if not FileUtils.is_excluded(os.path.join(root, d))]
            if FileUtils.is_excluded(root): continue

            curr_proj = None
            temp = root
            while temp.startswith(Config.ROOT_DIR):
                if temp in self.project_map: curr_proj = self.project_map[temp]; break
                temp = os.path.dirname(temp)
                if temp == os.path.dirname(temp): break
            if not curr_proj: continue

            for f in files:
                if not f.endswith('.md'): continue
                path = os.path.join(root, f)
                lines = FileUtils.read_file(path)
                if not lines: continue

                mod = False
                fname = os.path.splitext(f)[0]
                i = 0
                
                # è¾…åŠ©å‡½æ•°ï¼šè·å–å¿½ç•¥ Callout æ ‡è®°çš„åŸå§‹ç¼©è¿›
                def get_raw_indent(s):
                    # å…ˆç§»é™¤å¼•ç”¨æ ‡è®°ï¼Œç„¶åå»é™¤å·¦ä¾§ç©ºç™½ä»¥è®¡ç®—ç¼©è¿›
                    # re.sub(r'^>\s?', '', s) å¤„ç†å¼€å¤´çš„ '> ' æˆ– '>'
                    return len(s) - len(re.sub(r'^>\s?', '', s).lstrip())

                while i < len(lines):
                    line = lines[i]
                    # [æ ¸å¿ƒ] æ£€æµ‹ä»»ä½•ä»»åŠ¡æ ‡è®°ï¼ˆè£¸ä»»åŠ¡æˆ–å¸¦æ—¥æœŸçš„ï¼‰
                    # æ£€æŸ¥ - [ ] æ¨¡å¼ï¼ˆå…è®¸ > å‰ç¼€ï¼‰
                    if not re.match(r'^[\s>]*-\s*\[.\]', line):
                        i += 1
                        continue
                        
                    # --- å‘ç°ä»»åŠ¡ ---
                    
                    # 1. æ—¥æœŸæ£€æµ‹
                    task_date = None
                    date_match = re.search(r'[ğŸ“…âœ…]\s*(\d{4}-\d{2}-\d{2})', line)
                    if date_match:
                        task_date = date_match.group(1)
                    else:
                        # å°è¯•æ–°é“¾æ¥æ ¼å¼
                        link_match = re.search(r'\[\[(\d{4}-\d{2}-\d{2})(?:#|\]\])', line)
                        if link_match: task_date = link_match.group(1)
                    
                    # [è‡ªåŠ¨è¡¥å…¨] å¦‚æœæ˜¯è£¸ä»»åŠ¡ï¼ˆæœªæ‰¾åˆ°æ—¥æœŸï¼‰ï¼Œé»˜è®¤ä¸ºä»Šå¤©
                    if not task_date:
                        task_date = today_str
                        mod = True
                        # Logger.info(f"Captured Naked Task in {fname}")
                        
                    # 2. Block ID
                    # 2. å— ID
                    # ä¸¥æ ¼ ID æ­£åˆ™ï¼šç©ºæ ¼ + ^ + 6-7 ä¸ªå­—æ¯æ•°å­— + ç»“å°¾
                    id_m = re.search(r'(?<=\s)\^([a-zA-Z0-9]{6,7})\s*$', line)
                    if id_m: bid = id_m.group(1)
                    else:
                        # å›é€€ / è‡ªåŠ¨ç”Ÿæˆ
                        bid = self.generate_block_id().replace('^', '')
                        mod = True
                        
                    # 3. å±æ€§ä¸è§£æ
                    indent = get_raw_indent(line)
                    status_match = re.search(r'-\s*\[(.)\]', line)
                    st = status_match.group(1) if status_match else ' '
                    clean_txt = self.clean_task_text(line, bid, context_name=fname)
                    
                    # æå–æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆç°æœ‰é€»è¾‘ï¼‰
                    dates = " ".join(re.findall(r'([ğŸ“…âœ…]\s*\d{4}-\d{2}-\d{2}|\[\[\d{4}-\d{2}-\d{2}#\^[a-zA-Z0-9]+\|ğŸ“…\]\]|\[\[\d{4}-\d{2}-\d{2}\]\])', line))
                    
                    # [è‡ªåŠ¨è¡¥å…¨] å¦‚æœæˆ‘ä»¬æ¨æ–­äº†æ—¥æœŸï¼Œç¡®ä¿æ—¥æœŸé“¾æ¥å­˜åœ¨
                    # ä»…å½“æ—¥æœŸå°šæœªä»¥æŸç§å½¢å¼å‡ºç°åœ¨æ–‡æœ¬ä¸­æ—¶æ‰è¿½åŠ 
                    if task_date not in line: 
                        if not dates: dates = f"[[{task_date}]]"
                        else: dates += f" [[{task_date}]]"
                        mod = True 

                    # 4. æ ¼å¼åŒ–è¡Œå¹¶æ£€æŸ¥æ›´æ–°
                    new_line = self.format_line(indent, st, clean_txt, dates, fname, bid, False)
                    
                    # [ä¿®å¤] Callout ä¿æŠ¤ / è­¦å«
                    # å¦‚æœé€šè¿‡åŸæœ¬æœ‰å¼•ç”¨ï¼Œç¡®ä¿ new_line ä¹Ÿæœ‰å¼•ç”¨
                    is_quoted = line.strip().startswith('>')
                    if is_quoted:
                        if not new_line.strip().startswith('>'):
                            # å‰ç½® > å¹¶ç¡®ä¿é—´è·
                            # å¦‚æœ new_line æ˜¯ "\t- ..."ï¼Œä½¿å…¶å˜ä¸º "> \t- ..."
                            # ç®€å•å‰ç½® "> " æ˜¯æ ‡å‡† Obsidian è¯­æ³•
                            new_line = "> " + new_line
                    
                    if new_line.strip() != line.strip():
                        lines[i] = new_line
                        mod = True
                        
                    # 5. æ•è·ä¸å“ˆå¸Œ
                    block, consumed = self.capture_block(lines, i)
                    combined_text = clean_txt + "|||" + self.normalize_block_content(block[1:])
                    content_hash = self.sm.calc_hash(st, combined_text)
                    
                    # 6. å­˜å‚¨
                    if task_date not in source_data_by_date: source_data_by_date[task_date] = {}
                    source_data_by_date[task_date][bid] = {
                        'proj': curr_proj, 'bid': bid, 'pure': clean_txt, 'status': st,
                        'path': path, 'fname': fname, 'raw': block, 'hash': content_hash, 'indent': indent,
                        'dates': dates, 'is_quoted': is_quoted
                    }
                    
                    i += consumed
                
                if mod:
                    # å†™å…¥å‰æ¸…ç† Callout
                    lines = self.inject_into_callout(lines, [])
                    lines = self.cleanup_empty_callouts(lines)
                    FileUtils.write_file(path, lines)

        return source_data_by_date

    def process_date(self, target_date, src_tasks_for_date):
        today_str = datetime.date.today().strftime('%Y-%m-%d')
        is_past = target_date < today_str

        daily_path = os.path.join(Config.DAILY_NOTE_DIR, f"{target_date}.md")

        if os.path.exists(daily_path):
            self.organize_orphans(daily_path, target_date)

        dn_tasks = {}
        new_dn_tasks = []
        dn_lines = []
        if os.path.exists(daily_path):
            dn_lines = FileUtils.read_file(daily_path) or []
            curr_ctx = None
            i = 0
            while i < len(dn_lines):
                line = dn_lines[i]
                h_m = re.match(r'^##\s*\[\[(.*?)\]\]', line.strip())
                if h_m: curr_ctx = h_m.group(1); i += 1; continue

                tm = re.match(r'^[\s>]*-\s*\[(.)\]', line)
                if tm:
                    lm = re.search(r'\[\[(.*?)\#\^([a-zA-Z0-9]{6,})\|.*?\]\]', line)
                    if lm:
                        ctx_name = lm.group(1)
                        bid = lm.group(2)
                        raw, c = self.capture_block(dn_lines, i)
                        clean = self.clean_task_text(line, bid, context_name=ctx_name)
                        st = tm.group(1)
                        # å¼€å§‹ä¿®å¤ï¼šåœ¨å“ˆå¸Œä¸­åŒ…å«å­ä»»åŠ¡
                        combined_text = clean + "|||" + self.normalize_block_content(raw[1:])
                        content_hash = self.sm.calc_hash(st, combined_text)
                        # ç»“æŸä¿®å¤
                        dn_tasks[bid] = {
                            'pure': clean, 'status': st, 'idx': i, 'len': c,
                            'raw': raw, 'hash': content_hash
                        }
                        i += c;
                        continue
                    elif curr_ctx and curr_ctx in self.project_path_map:
                        if '^' not in line:
                             # [ä¿®å¤] æ¯æ—¥ç¬”è®°ç¼©è¿›è®¡ç®—ï¼ˆç§»é™¤ > å³ä½¿ä¸å¯è§ï¼‰
                             # è™½ç„¶ Daily å¾ˆå°‘æœ‰ >ï¼Œä½†ä»¥é˜²ä¸‡ä¸€
                            raw_indent = len(line) - len(line.lstrip())
                             
                            raw, c = self.capture_block(dn_lines, i)
                            new_dn_tasks.append({
                                'proj': curr_ctx, 'idx': i, 'len': c, 'raw': raw,
                                'st': tm.group(1), 'indent': raw_indent
                            })
                            i += c;
                            continue
                        else:
                            # å›é€€ï¼šæ£€æŸ¥è¯¥è¡Œæ˜¯å¦æœ‰æŒ‡å‘å·²çŸ¥é¡¹ç›®æˆ–æ–‡ä»¶çš„ç›´æ¥é“¾æ¥
                            link_match = re.search(r'\[\[(.*?)(?:#|\||\]\])', line)
                            if link_match:
                                pot = link_match.group(1).strip()
                                pot = unicodedata.normalize('NFC', pot)
                                # æ£€æŸ¥é¡¹ç›®æ˜ å°„ï¼Œç„¶åæ£€æŸ¥æ–‡ä»¶æ˜ å°„
                                target_file = None
                                if pot in self.project_path_map: target_file = self.project_path_map[pot]
                                elif pot in self.file_path_map: target_file = self.file_path_map[pot]
                                
                                if target_file:
                                    raw_indent = len(line) - len(line.lstrip())
                                    raw, c = self.capture_block(dn_lines, i)
                                    new_dn_tasks.append({
                                        # å¦‚æœå¯ç”¨ï¼Œä½¿ç”¨ 'proj' é”®ä½œä¸ºé¡¹ç›®åç§°ï¼Œæˆ–è€…ä»…ä½¿ç”¨æ–‡ä»¶å
                                        'proj': self.project_map.get(os.path.dirname(target_file), pot), 
                                        'idx': i, 'len': c, 'raw': raw,
                                        'st': tm.group(1), 'indent': raw_indent
                                    })
                                    i += c;
                                    continue
                i += 1

        dn_mod = False
        if new_dn_tasks:
            Logger.info(f"å¤„ç†æ–°å»ºä»»åŠ¡: {len(new_dn_tasks)} æ¡", target_date)
            for nt in reversed(new_dn_tasks):
                p_name = nt['proj']
                txt = nt['raw'][0]
                clean = self.clean_task_text(txt)
                tgt = self.extract_routing_target(txt) or self.project_path_map.get(p_name)
                if not tgt: continue
                bid = self.generate_block_id().replace('^', '')
                fname = os.path.splitext(os.path.basename(tgt))[0]

                s_l = self.format_line(nt['indent'], nt['st'], clean, f"ğŸ“… {target_date}", fname, bid, False)
            
            # [ä¿®å¤] æºå—ï¼šä½œä¸ºå¼•ç”¨
            s_children = self.normalize_child_lines(nt['raw'][1:], nt['indent'], as_quoted=True)
            s_blk = [s_l] + s_children
            
            d_l = self.format_line(nt['indent'], nt['st'], clean, "", fname, bid, True)
            
            # [ä¿®å¤] æ¯æ—¥å—ï¼šçº¯æ–‡æœ¬
            d_children = self.normalize_child_lines(nt['raw'][1:], nt['indent'], as_quoted=False)
            d_blk = [d_l] + d_children

            dn_lines[nt['idx']:nt['idx'] + nt['len']] = d_blk
            dn_mod = True

            sl = FileUtils.read_file(tgt) or []
            sl = self.inject_into_callout(sl, s_blk)
            
            # æ¢é’ˆ 1: æ³¨å…¥æ–°ä»»åŠ¡
            Logger.debug_block(f"Injecting New Task into {fname}", s_blk)
            
            FileUtils.write_file(tgt, sl)
            
            # è§¦å‘å»¶è¿ŸéªŒè¯
            self.trigger_delayed_verification(tgt)

            # å¼€å§‹ä¿®å¤ï¼šåœ¨å“ˆå¸Œä¸­åŒ…å«å­ä»»åŠ¡
            combined_text = clean + "|||" + self.normalize_block_content(nt['raw'][1:])
            h = self.sm.calc_hash(nt['st'], combined_text)
            # ç»“æŸä¿®å¤
            self.sm.update_task(bid, h, tgt)

            # ä¸ºæ¯ä¸ªæ–°ä»»åŠ¡æ’å…¥è§¦å‘ç½®é¡¶
            # è™½ç„¶ä¸Šé¢è°ƒç”¨äº† inject_into_calloutï¼Œä½†ç”¨ç©ºåˆ—è¡¨è°ƒç”¨å®ƒå¯ä»¥ç¡®ä¿
            # å¦‚æœ Callout ä¸åœ¨é¡¶éƒ¨ï¼ˆä¾‹å¦‚æ—§æ–‡ä»¶ï¼‰ï¼Œå®ƒä¼šç§»åŠ¨åˆ°é‚£é‡Œã€‚
            # ä½†ç­‰ç­‰ï¼Œä¸Šé¢çš„è°ƒç”¨ `sl = self.inject_into_callout(sl, s_blk)` å·²ç»åšäº†è¿™ä¸ªã€‚
            # æ‰€ä»¥è¿™é‡Œä¸éœ€è¦é¢å¤–çš„è°ƒç”¨ã€‚


        if dn_mod:
            FileUtils.write_file(daily_path, dn_lines)
            self.sm.save()
            return

        src_tasks = src_tasks_for_date
        all_ids = set(src_tasks.keys()) | set(dn_tasks.keys())
        append_to_dn = {}
        src_updates = {}
        src_deletes = {}

        for bid in all_ids:
            in_s = bid in src_tasks
            in_d = bid in dn_tasks
            last_hash = self.sm.get_task_hash(bid)

            if in_s and in_d:
                sd = src_tasks[bid]
                dd = dn_tasks[bid]
                s_changed = (sd['hash'] != last_hash)
                d_changed = (dd['hash'] != last_hash)

                if s_changed and not d_changed:
                    blk = self.reconstruct_daily_block(sd, target_date)
                    
                    # æ¢é’ˆ 2: ä»æºæ›´æ–°æ¯æ—¥ç¬”è®°
                    Logger.debug_block(f"Updating Daily Note from Source {sd['fname']}", blk)
                    
                    dn_lines[dd['idx']:dd['idx'] + dd['len']] = blk
                    dn_mod = True
                    self.sm.update_task(bid, sd['hash'], sd['path'])

                elif d_changed and not s_changed:
                    n_l = self.format_line(sd['indent'], dd['status'], dd['pure'], f"ğŸ“… {target_date}", sd['fname'], bid,
                                           False)
                    was_quoted = sd.get('is_quoted', False)
                    if was_quoted and not n_l.strip().startswith('>'): n_l = f"> {n_l}"

                    # [ä¿®å¤] ä¸ºæºæ ‡å‡†åŒ–å­é¡¹ (as_quoted=True)
                    blk = [n_l] + self.normalize_child_lines(dd['raw'][1:], sd['indent'], as_quoted=True)

                    if sd['path'] not in src_updates: src_updates[sd['path']] = {}
                    
                    # æ¢é’ˆ 3: å‡†å¤‡æºæ›´æ–°ï¼ˆæ‰¹å¤„ç†æ—¶è®°å½•ï¼Œä½†ä¹Ÿåœ¨æ­¤å¤„è®°å½•å†…å®¹ï¼Ÿï¼‰
                    # ç™½ç›’ï¼šæ˜¾ç¤ºæˆ‘ä»¬æ­£åœ¨æ’é˜Ÿçš„å†…å®¹ã€‚
                    Logger.debug_block(f"Queueing Update for Source {sd['fname']}", blk)
                    
                    src_updates[sd['path']][bid] = blk
                    self.sm.update_task(bid, dd['hash'], sd['path'])

                elif s_changed and d_changed:
                    if sd['hash'] != dd['hash']:
                        Logger.info(f"å†²çªæ£€æµ‹ {bid}: ä¼˜å…ˆä¿ç•™ Daily ä¿®æ”¹", target_date)
                        n_l = self.format_line(sd['indent'], dd['status'], dd['pure'], f"ğŸ“… {target_date}", sd['fname'],
                                               bid, False)
                        was_quoted = sd.get('is_quoted', False)
                        if was_quoted and not n_l.strip().startswith('>'): n_l = f"> {n_l}"

                        # [ä¿®å¤] ä¸ºæºæ ‡å‡†åŒ–å­é¡¹ (as_quoted=True)
                        blk = [n_l] + self.normalize_child_lines(dd['raw'][1:], sd['indent'], as_quoted=True)

                        if sd['path'] not in src_updates: src_updates[sd['path']] = {}
                        src_updates[sd['path']][bid] = blk
                        self.sm.update_task(bid, dd['hash'], sd['path'])
                else:
                    if last_hash is None: self.sm.update_task(bid, sd['hash'], sd['path'])

            elif in_s and not in_d:
                sd = src_tasks[bid]
                
                # æ£€æŸ¥ 1: å†å²ä¿æŠ¤ï¼ˆè¿‡å»æ—¥æœŸï¼‰-> å¼ºåˆ¶æ¢å¤
                if is_past:
                    Logger.info(f"å†å²ä¿æŠ¤: æ£€æµ‹åˆ°æ—§æ—¥è®°({target_date})ç¼ºå¤±ä»»åŠ¡ {bid}ï¼Œå¼ºåˆ¶å›å†™", target_date)
                    if sd['proj'] not in append_to_dn: append_to_dn[sd['proj']] = []
                    append_to_dn[sd['proj']].append(sd)
                    self.sm.update_task(bid, sd['hash'], sd['path'])

                # æ£€æŸ¥ 2: æ­£å¸¸åˆ é™¤ï¼ˆä»Šå¤©æˆ–æœªæ¥ï¼‰-> å…è®¸åˆ é™¤
                elif last_hash:
                    Logger.info(f"ç¦»çº¿åŒæ­¥: æ£€æµ‹åˆ° Daily åˆ é™¤ {bid}ï¼Œç§»é™¤ Source", target_date)
                    if sd['path'] not in src_deletes: src_deletes[sd['path']] = {}
                    src_deletes[sd['path']][bid] = sd['path']
                    self.sm.remove_task(bid)

                # æ£€æŸ¥ 3: æºä¸­çš„æ–°ä»»åŠ¡ -> æ·»åŠ åˆ° Daily
                else:
                    if sd['proj'] not in append_to_dn: append_to_dn[sd['proj']] = []
                    append_to_dn[sd['proj']].append(sd)
                    self.sm.update_task(bid, sd['hash'], sd['path'])

            elif in_d and not in_s:
                dd = dn_tasks[bid]
                if last_hash:
                    Logger.info(f"ç¦»çº¿åŒæ­¥: æ£€æµ‹åˆ° Source åˆ é™¤ {bid}ï¼Œç§»é™¤ Daily", target_date)
                    for k in range(dd['idx'], dd['idx'] + dd['len']): dn_lines[k] = "__DEL__\n"
                    dn_mod = True
                    self.sm.remove_task(bid)
                else:
                    self.sm.update_task(bid, dd['hash'], "UNKNOWN")

        if dn_mod: dn_lines = [x for x in dn_lines if x != "__DEL__\n"]

        for path, bids in src_deletes.items():
            sl = FileUtils.read_file(path)
            if not sl: continue
            out, i, chg = [], 0, False
            deleted_bids = list(bids.keys())
            while i < len(sl):
                im = re.search(r'\^([a-zA-Z0-9]{6,})\s*$', sl[i])
                if not im: im = re.search(r'\(connect::.*?\^([a-zA-Z0-9]{6,})\)', sl[i])
                if im and im.group(1) in deleted_bids:
                    _, c = self.capture_block(sl, i);
                    i += c;
                    chg = True
                else:
                    out.append(sl[i]); i += 1
            if chg:
                # å³ä½¿åˆ é™¤ä¹Ÿå¼ºåˆ¶ç½®é¡¶ï¼ˆä»¥ç§»åŠ¨å‰©ä½™ä»»åŠ¡ï¼‰
                stem = os.path.splitext(os.path.basename(path))[0]
                out = self.inject_into_callout(out, [], stem)
                out = self.cleanup_empty_callouts(out)
                
                # [ä¿®å¤ 1] å‡€åŒ–åˆ—è¡¨
                out = [l for l in out if l is not None]
                
                FileUtils.write_file(path, out)

        for path, ups in src_updates.items():
            sl = FileUtils.read_file(path)
            if not sl: continue
            out, i, chg = [], 0, False
            while i < len(sl):
                im = re.search(r'\^([a-zA-Z0-9]{6,})\s*$', sl[i])
                if not im: im = re.search(r'\(connect::.*?\^([a-zA-Z0-9]{6,})\)', sl[i])
                if im and im.group(1) in ups:
                    _, c = self.capture_block(sl, i)
                    out.extend(ups[im.group(1)])
                    i += c;
                    chg = True
                else:
                    out.append(sl[i]); i += 1
            if chg:
                # å¼ºåˆ¶ç½®é¡¶ï¼ˆé‡æ–°æ³¨å…¥ç©ºï¼‰ç„¶åæ¸…ç†
                stem = os.path.splitext(os.path.basename(path))[0]
                out = self.inject_into_callout(out, [], stem)
                out = self.cleanup_empty_callouts(out)
                
                # æ¢é’ˆ 4: æ‰¹é‡æºæ›´æ–°
                Logger.debug_block(f"Batch Updating Source {os.path.basename(path)}", out)
                
                # [ä¿®å¤ 1] å‡€åŒ–åˆ—è¡¨ï¼šç§»é™¤ None å€¼ä»¥é˜²æ­¢å´©æºƒ
                out = [l for l in out if l is not None]
                
                FileUtils.write_file(path, out)
                self.trigger_delayed_verification(path)
                Logger.info(f"æºå†…å®¹åŒæ­¥: {os.path.basename(path)}", target_date)

        if append_to_dn:
            try:
                j_idx = next(i for i, l in enumerate(dn_lines) if l.strip() == "# Journey")
            except:
                j_idx = 0
            end_pt = len(dn_lines)
            for i in range(j_idx + 1, len(dn_lines)):
                if dn_lines[i].startswith('# '): end_pt = i; break

            offset = 0
            for proj, tasks in append_to_dn.items():
                header = f"## [[{proj}]]"
                h_idx = -1
                curr_search_end = end_pt + offset
                for k in range(j_idx, curr_search_end):
                    if dn_lines[k].strip() == header: h_idx = k; break

                txt_blk = []
                for t in tasks:
                    l1 = self.format_line(t['indent'], t['status'], t['pure'], "", t['fname'], t['bid'], True)
                    # [ä¿®å¤] ä¸º Daily æ ‡å‡†åŒ–å­é¡¹ (as_quoted=False)
                    children = self.normalize_child_lines(t['raw'][1:], t['indent'], as_quoted=False)
                    txt_blk.extend([l1] + children)
                    if not txt_blk[-1].endswith('\n'): txt_blk[-1] += '\n'

                if h_idx != -1:
                    ins = curr_search_end
                    for k in range(h_idx + 1, curr_search_end):
                        if dn_lines[k].startswith('#'): ins = k; break
                    dn_lines[ins:ins] = txt_blk
                    offset += len(txt_blk)
                else:
                    chunk = [f"\n{header}\n"] + txt_blk
                    dn_lines[end_pt + offset:end_pt + offset] = chunk
                    offset += len(chunk)

            if offset > 0: dn_mod = True

        dn_lines, cleaned = self.cleanup_empty_headers(dn_lines, target_date)
        if cleaned: dn_mod = True
        
        # --- [ä¿®å¤å¼€å§‹] é˜²æ­¢ Ping-Pong å¾ªç¯ ---
        # å¼ºåˆ¶æ¸…ç† Daily Note çš„å°¾éƒ¨
        original_len = len(dn_lines)
        dn_lines = self.aggressive_daily_clean(dn_lines)
        if len(dn_lines) != original_len:
            dn_mod = True # ç¡®ä¿å¦‚æœæˆ‘ä»¬æ¸…ç†äº†æŸäº›å†…å®¹åˆ™è¿›è¡Œä¿å­˜
        # --- [ä¿®å¤ç»“æŸ] ---

        if dn_mod:
            # [ä¿®å¤ 1] åŒæ ·å‡€åŒ– Daily Note è¡Œï¼Œä»¥é˜²ä¸‡ä¸€
            dn_lines = [l for l in dn_lines if l is not None]

            FileUtils.write_file(daily_path, dn_lines)
            Logger.info("Daily Note æ›´æ–°å®Œæˆ", target_date)

        self.sm.save()

