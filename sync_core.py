import os
import re
import random
import string
import unicodedata
import datetime
import threading
import time
from typing import Dict, List, Optional, Any, Set
from config import Config
from utils import Logger, FileUtils


class SyncCore:
    def __init__(self, state_manager):
        self.sm = state_manager
        self.project_map = {}
        self.project_path_map = {}
        self.file_path_map = {}

    def trigger_delayed_verification(self, filepath, delay=10):
        def _job():
            time.sleep(delay)
            content = FileUtils.read_file(filepath) or []
            Logger.debug_block(f"VERIFICATION (T+{delay}s) Snapshot: {os.path.basename(filepath)}", content)

        t = threading.Thread(target=_job, daemon=True)
        t.start()

    def generate_block_id(self):
        return '^' + ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))

    def parse_yaml_tags(self, lines):
        tags = []
        if not lines or lines[0].strip() != '---': return []
        in_yaml = False
        for i, line in enumerate(lines):
            if i == 0: in_yaml = True; continue
            if line.strip() == '---': break
            if in_yaml and ('tags:' in line or 'main' in line):
                if re.search(r'\bmain\b', line): tags.append('main')
        return tags

    def clean_task_text(self, line, block_id=None, context_name=None):
        """
        [v10.7 Aggressive Clean]
        å¢åŠ é€šç”¨å°¾éƒ¨æ¸…ç†ï¼Œé˜²æ­¢æ•‘æ´æ¨¡å¼ä¸‹æ®‹ç¼º ID (å¦‚ ^04cn) æœªè¢«æ¸…é™¤å¯¼è‡´ ID é‡å¤ã€‚
        [v10.8 Time Strip] å¢åŠ æ—¶é—´æ®µå‰¥ç¦»ï¼Œé˜²æ­¢ 07:00 - 11:20 æ±¡æŸ“æºæ–‡ä»¶ã€‚
        """
        # 1. ç§»é™¤ Checkbox (ä¿ç•™)
        line = re.sub(r'^\s*-\s*\[.\]\s?', '', line)

        # === [æ–°å¢] 2. ç§»é™¤ Day Planner æ—¶é—´æ®µ ===
        # åŒ¹é…: "07:00 " æˆ– "07:00 - 11:20 "
        # é€»è¾‘: åªæœ‰ä½äºè¡Œé¦–ï¼ˆå»é™¤ checkbox åï¼‰çš„æ—¶é—´æ‰ä¼šè¢«è§†ä¸ºè°ƒåº¦ä¿¡æ¯
        line = re.sub(r'^\s*\d{1,2}:\d{2}(?:\s*-\s*\d{1,2}:\d{2})?\s+', '', line)

        clean_text = line

        # 3. ç§»é™¤æŒ‡å®šå— ID (åŸé€»è¾‘)
        if block_id:
            clean_text = re.sub(rf'(?<=\s)\^{re.escape(block_id)}\s*$', '', clean_text)

        # 4. é€šç”¨å°¾éƒ¨æ¸…ç† (åŸé€»è¾‘)
        clean_text = re.sub(r'\s+\^[a-zA-Z0-9]*$', '', clean_text)

        # 5. ç§»é™¤æ—¥æœŸé“¾æ¥ (åŸé€»è¾‘)
        clean_text = re.sub(r'\[\[\d{4}-\d{2}-\d{2}(?:#\^[a-zA-Z0-9]+)?(?:\|.*?)?\]\]', '', clean_text)

        # 6. ç§»é™¤æ–‡ä»¶è‡ªèº«é“¾æ¥ (åŸé€»è¾‘)
        if context_name:
            clean_text = re.sub(rf'\[\[{re.escape(context_name)}(?:#\^[a-zA-Z0-9]+)?(?:\|.*?)?\]\]', '', clean_text)

        # 7. ç§»é™¤ Emoji æ—¥æœŸ (åŸé€»è¾‘)
        clean_text = re.sub(r'[ğŸ“…âœ…]\s*\d{4}-\d{2}-\d{2}', '', clean_text)
        clean_text = re.sub(r'\(connect::.*?\)', '', clean_text)

        return clean_text.strip()

    def normalize_block_content(self, block_lines):
        normalized = []
        for line in block_lines:
            clean = re.sub(r'^[\s>]+', '', line).strip()
            if not clean or clean in ['-', '- ']: continue
            normalized.append(clean)
        return "\n".join(normalized) + "\n"

    def extract_routing_target(self, line):
        clean = re.sub(r'\[\[[^\]]*?\#\^[a-zA-Z0-9]{6,}\|[âš“\*ğŸ”—â®ğŸ“…]\]\]', '', line)
        matches = re.findall(r'\[\[(.*?)\]\]', clean)
        for match in matches:
            pot = match.split('|')[0]
            pot = unicodedata.normalize('NFC', pot)
            if pot in self.file_path_map: return self.file_path_map[pot]
        return None

    # === [NEW] Helper for robust indentation ===
    def _get_indent_depth(self, line):
        """
        [Helper] ç»Ÿä¸€è®¡ç®—ç¼©è¿›è§†è§‰æ·±åº¦ (Tab=4 spaces)
        è§£å†³ Tab/Space æ··ç”¨å¯¼è‡´çš„å±‚çº§åˆ¤æ–­å¤±æ•ˆé—®é¢˜ã€‚
        """
        no_quote = re.sub(r'^>\s?', '', line)
        expanded = no_quote.expandtabs(4)
        return len(expanded) - len(expanded.lstrip())

    # === [MODIFIED] Robust Capture Block ===
    def capture_block(self, lines, start_idx):
        """
        [v14.2 Indent-Priority Capture]
        ä¿®å¤åŒé‡ç¼©è¿›ä»»åŠ¡ (- [ ]) è¢«æˆªæ–­çš„ Bugã€‚
        æ ¸å¿ƒé€»è¾‘å˜æ›´ï¼šç¡®ç«‹ã€ç¼©è¿›éœ¸æƒã€‘ã€‚
        åªè¦å½“å‰è¡Œç¼©è¿› > çˆ¶çº§ç¼©è¿›ï¼Œæ— æ¡ä»¶è§†ä¸ºå­å†…å®¹ï¼Œè·³è¿‡ä»»ä½•å†…å®¹æ£€æŸ¥ï¼ˆå¦‚ # æˆ– ---ï¼‰ã€‚
        åªæœ‰ç¼©è¿› <= çˆ¶çº§æ—¶ï¼Œæ‰è¿›è¡Œç»“æŸåˆ¤å®šã€‚
        """
        if start_idx >= len(lines): return [], 0

        # 1. è·å–çˆ¶çº§ï¼ˆé”šç‚¹ï¼‰çš„è§†è§‰ç¼©è¿›æ·±åº¦
        base_depth = self._get_indent_depth(lines[start_idx])

        block = [lines[start_idx]]
        consumed = 1
        j = start_idx + 1

        while j < len(lines):
            nl = lines[j]

            # 1. ç©ºè¡Œå¤„ç†ï¼šå§‹ç»ˆä¿ç•™ï¼Œä¸ä½œä¸ºåˆ¤å®šä¾æ®
            if not nl.strip():
                block.append(nl)
                consumed += 1
                j += 1
                continue

            # 2. å¼ºåˆ†éš”ç¬¦ï¼šå”¯ä¸€çš„ä¾‹å¤–ï¼Œå¿…é¡»æˆªæ–­
            if nl.strip() == '----------': break

            # 3. è®¡ç®—å½“å‰è¡Œç¼©è¿›
            curr_depth = self._get_indent_depth(nl)

            # === [æ ¸å¿ƒä¿®å¤] ç¼©è¿›ä¼˜å…ˆåŸåˆ™ ===
            # å¦‚æœå½“å‰è¡Œæ¯”çˆ¶çº§ç¼©è¿›æ·±ï¼Œå®ƒå°±æ˜¯å­å…ƒç´ ã€‚
            # ä¸æ£€æŸ¥å®ƒæ˜¯å¦ä»¥ '#' å¼€å¤´ï¼Œä¹Ÿä¸åšä»»ä½•æ­£åˆ™æ¸…æ´—ã€‚
            # è¿™ä¿è¯äº† `      - [ ]` è¿™ç§ç»“æ„ç»å¯¹ä¼šè¢«æ•è·ã€‚
            if curr_depth > base_depth:
                block.append(nl)
                consumed += 1
                j += 1
                continue

            # 4. åªæœ‰å½“ç¼©è¿› <= çˆ¶çº§æ—¶ï¼Œæ‰è§†ä¸ºæ½œåœ¨çš„ç»“æŸ
            # æ­¤æ—¶é‡åˆ°åŒçº§ä»»åŠ¡ã€åŒçº§æ ‡é¢˜æˆ–æ›´æµ…çš„å†…å®¹ï¼Œå‡ç»“æŸæ•è·
            break

        return block, consumed

    def normalize_raw_tasks(self, lines, filename_stem):
        if not lines or not filename_stem: return lines

        new_lines = []
        today_str = datetime.date.today().strftime("%Y-%m-%d")
        raw_pattern = re.compile(r'^(>\s*-\s*\[\s*\])(.*)$')
        id_pattern = re.compile(r'\^[a-z0-9]{6}\s*$')

        def generate_id():
            return ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))

        for line in lines:
            match = raw_pattern.match(line)
            if match:
                prefix = match.group(1)
                text_body = match.group(2).strip()

                if not id_pattern.search(text_body):
                    new_id = generate_id()
                    formatted_body = f"[[{filename_stem}#^{new_id}|â®]] [[{today_str}]]"
                    if text_body:
                        formatted_body += f" {text_body}"
                    new_lines.append(f"{prefix} {formatted_body} ^{new_id}")
                else:
                    new_lines.append(line)
            else:
                new_lines.append(line)
        return new_lines

    def maintain_section_integrity(self, lines):
        cleaned = []
        empty_count = 0
        for line in lines:
            if not line.strip():
                empty_count += 1
                if empty_count <= 1:
                    cleaned.append(line)
            else:
                empty_count = 0
                cleaned.append(line)
        return cleaned

    def _calculate_sort_key(self, block_data):
        """
        [v12.2 Absolute Hybrid Sort]
        ç»å¯¹æ’åºè§„åˆ™ï¼š
        1. ç¬¬ä¸€æ¢¯é˜Ÿï¼šæœ‰æ—¶é—´é™åˆ¶çš„ä»»åŠ¡ (Time-Blocked) -> æŒ‰æ—¶é—´å…ˆå (08:00 < 09:00)
        2. ç¬¬äºŒæ¢¯é˜Ÿï¼šæ— æ—¶é—´é™åˆ¶çš„ä»»åŠ¡ -> æŒ‰ Block ID å­—å…¸åº (^aaaa < ^zzzz)
        """
        first_line = block_data['lines'][0].strip()
        block_id = block_data['id']

        # --- 1. æ—¶é—´æå– ---
        time_match = re.search(r'(\d{1,2}:\d{2})', first_line)

        if time_match:
            has_time = 0
            time_val = time_match.group(1).zfill(5)
        else:
            has_time = 1
            time_val = "99:99"

        return (has_time, time_val, block_id)

    def inject_into_task_section(self, file_lines, block_lines, filename_stem=None):
        """
        [v14.5 Indent-Aware Injection]
        ä¿®å¤ inject é€»è¾‘è¯¯å°†ç¼©è¿›çš„å­ä»»åŠ¡ (- [ ]) è¯†åˆ«ä¸ºæ–° Block å¯¼è‡´çš„æˆªæ–­é—®é¢˜ã€‚
        ç°åœ¨åªæœ‰ã€é¡¶å±‚ä»»åŠ¡ã€‘(ç¼©è¿› < 2 ç©ºæ ¼) æ‰ä¼šè§¦å‘åˆ†å—ã€‚
        """
        # --- 1. å®šä½é”šç‚¹ ---
        start_idx = -1
        end_idx = -1
        for i, line in enumerate(file_lines):
            if line.strip() == '# Tasks': start_idx = i; break

        if start_idx != -1:
            for i in range(start_idx + 1, len(file_lines)):
                curr_line = file_lines[i].strip()
                if curr_line == '----------': end_idx = i; break
            if end_idx <= start_idx: end_idx = -1

        # --- 2. è‡ªæ„ˆç»“æ„ ---
        need_scaffold = False
        if start_idx == -1:
            need_scaffold = True
            file_lines = [l for l in file_lines if l.strip() not in ('# Tasks', '----------')]
        elif end_idx == -1:
            file_lines.append("\n----------\n")
            end_idx = len(file_lines) - 1
        elif end_idx < start_idx:
            need_scaffold = True
            file_lines = [l for l in file_lines if l.strip() not in ('# Tasks', '----------')]

        if need_scaffold:
            insert_pos = 0
            if file_lines and file_lines[0].strip() == '---':
                for i in range(1, len(file_lines)):
                    if file_lines[i].strip() == '---': insert_pos = i + 1; break
            scaffold = ["\n", "# Tasks\n", "\n", "----------\n"]
            file_lines[insert_pos:insert_pos] = scaffold
            start_idx = insert_pos + 1
            end_idx = insert_pos + 3

        # --- 3. æå–ç°æœ‰å†…å®¹ ---
        existing_content = file_lines[start_idx + 1: end_idx]
        existing_structure_map = {}
        current_header_date = None
        header_pattern = re.compile(r'^#+\s*\[\[\s*(\d{4}-\d{2}-\d{2})\s*\]\]')
        id_pattern = re.compile(r'\^([a-zA-Z0-9]{6,})\s*$')

        for line in existing_content:
            stripped = line.strip()
            h_m = header_pattern.match(stripped)
            if h_m: current_header_date = h_m.group(1); continue
            if stripped.startswith('- ['):
                bid_m = id_pattern.search(stripped)
                if bid_m and current_header_date:
                    existing_structure_map[bid_m.group(1)] = current_header_date

        # --- 4. åˆå¹¶ä¸åˆ†ç»„ ---
        candidates = existing_content + block_lines
        blocks = []
        current_block = []
        date_pattern = re.compile(r'\[\[(\d{4}-\d{2}-\d{2})(?:#|\||\]\])')

        def flush_block(blk_lines):
            if not blk_lines: return
            head = blk_lines[0]
            bid_m = id_pattern.search(head)
            if bid_m:
                bid = bid_m.group(1)
                final_date = "0000-00-00"
                if bid in existing_structure_map:
                    final_date = existing_structure_map[bid]
                else:
                    date_m = date_pattern.search(head)
                    if date_m: final_date = date_m.group(1)
                blocks.append({'id': bid, 'date': final_date, 'lines': blk_lines})

        for line in candidates:
            s_line = line.strip()
            if not s_line: continue
            if s_line == '-----': continue
            if s_line == '----------': continue

            # å¤„ç†æ ‡é¢˜è¡Œï¼šå¼ºåˆ¶åˆ†å—
            if s_line.startswith('#'):
                flush_block(current_block);
                current_block = [];
                continue

            # å¤„ç†ä»»åŠ¡è¡Œï¼šå¢åŠ ç¼©è¿›æ£€æµ‹
            if s_line.startswith('- ['):
                # [å…³é”®ä¿®å¤] è®¡ç®—åŸå§‹ç¼©è¿›æ·±åº¦
                # ä¸ä½¿ç”¨ .strip() åçš„ s_lineï¼Œè€Œæ˜¯ä½¿ç”¨åŸå§‹ line
                # åªæœ‰ç¼©è¿›éå¸¸æµ… (å°äº2ä¸ªç©ºæ ¼æˆ–åŠä¸ªTab) çš„æ‰è§†ä¸ºæ–° Block
                # è¿™æ ·å¯ä»¥ä¿æŠ¤ç¼©è¿›çš„å­ä»»åŠ¡ (		- [ ]) ä¸è¢«æ‹†åˆ†

                # ç®€å•è®¡ç®—å‰å¯¼ç©ºç™½é•¿åº¦ (Tabç®—1ä¸ªå­—ç¬¦ï¼Œä½†åœ¨startswithé€»è¾‘ä¸‹è¶³å¤ŸåŒºåˆ†é¡¶å±‚)
                raw_indent_len = len(line) - len(line.lstrip())

                # å¦‚æœæ˜¯é¡¶å±‚ä»»åŠ¡ (Indent 0 or 1 space/tab usually 0)
                # ä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼ï¼šæ¯”å¦‚ < 2ã€‚
                # æ³¨æ„ï¼šå¦‚æœæ‚¨çš„é¡¶å±‚ä»»åŠ¡ä¹Ÿæœ‰ç¼©è¿›ï¼Œè¿™é‡Œéœ€è¦è°ƒæ•´ã€‚é€šå¸¸é¡¶å±‚ä»»åŠ¡æ˜¯è´´è¾¹çš„ã€‚
                is_toplevel = (raw_indent_len < 2)

                if is_toplevel:
                    flush_block(current_block)
                    current_block = [line]
                else:
                    # æ˜¯å­ä»»åŠ¡ï¼ŒåŠ å…¥å½“å‰å—
                    if current_block:
                        current_block.append(line)
                    # å¦‚æœæ²¡æœ‰ current_block (å³å­¤å„¿ç¼©è¿›ä»»åŠ¡)ï¼Œæš‚ä¸”ä½œä¸ºæ–°å—ï¼ˆè™½ç„¶ä¸åˆè§„èŒƒï¼‰
                    else:
                        current_block = [line]
            else:
                # çº¯æ–‡æœ¬æˆ–å…¶ä»–å†…å®¹ï¼Œå½’å±å½“å‰å—
                if current_block: current_block.append(line)

        flush_block(current_block)

        # --- 5. åˆ†ç»„ä¸æ’åº ---
        unique_map = {}
        for b in blocks: unique_map[b['id']] = b
        date_groups = {}
        for b in unique_map.values():
            d = b['date']
            if d not in date_groups: date_groups[d] = []
            date_groups[d].append(b)

        # [SORTING] æ‰§è¡Œç»å¯¹æ’åº
        for date_key, group_blocks in date_groups.items():
            group_blocks.sort(key=self._calculate_sort_key)

        # --- 6. æ„å»ºè¾“å‡º ---
        output_lines = []
        sorted_dates = sorted(date_groups.keys(), reverse=True)
        for d in sorted_dates:
            group_blocks = date_groups[d]
            if d and d != "0000-00-00":
                if output_lines: output_lines.append("\n")
                output_lines.append(f"## [[{d}]]\n")
                output_lines.append("\n")
            elif output_lines:
                output_lines.append("\n")
            for b in group_blocks:
                output_lines.extend(b['lines'])
                if output_lines and not output_lines[-1].endswith('\n'):
                    output_lines[-1] += '\n'

        section_body = ["\n"] + output_lines + ["\n"]

        # --- [FIX] ç§»é™¤å†…éƒ¨åˆ¤æ–­ï¼Œæ€»æ˜¯åº”ç”¨å˜æ›´åˆ° list ---
        file_lines[start_idx + 1: end_idx] = section_body
        return file_lines

    def aggressive_daily_clean(self, lines: list) -> list:
        if not lines: return []

        footer_idx = len(lines)
        for i, line in enumerate(lines):
            if line.strip().startswith('# Day planner') or line.strip().startswith('# Journey'):
                footer_idx = i
                break

        body = lines[:footer_idx]
        foot = lines[footer_idx:]

        cleaned_body = []
        empty_count = 0
        empty_pattern = re.compile(r'^\s*$')

        for i, line in enumerate(body):
            is_empty = bool(empty_pattern.fullmatch(line))
            if '---' in line: is_empty = False

            if is_empty:
                empty_count += 1
                if empty_count > 2:
                    Logger.debug(f"[CLEAN] Removing excess daily line {i + 1}: {repr(line)}")
                    continue
                else:
                    cleaned_body.append(line)
            else:
                empty_count = 0
                cleaned_body.append(line)

        return cleaned_body + foot

    def format_line(self, indent, status, text, dates, fname, bid, is_daily):
        # indent now represents visual depth (spaces)
        # We can simply output spaces, or convert to tabs if preferred.
        # Assuming we stick to spaces or mix based on indent // 4.
        # For robustness, we will just use indent spaces.
        # But original logic was: tab_count = indent // 4; indent_str = '\t' * tab_count
        # To maintain compatibility with visual depth:
        tab_count = indent // 4
        indent_str = '\t' * tab_count

        if is_daily:
            link = f"[[{fname}#^{bid}|â®]]"
            time_match = re.match(r'^(\d{1,2}:\d{2}(?:\s*-\s*\d{1,2}:\d{2})?)', text)
            if time_match:
                time_part = time_match.group(1)
                rest_part = text[len(time_part):].strip()
                return f"{indent_str}- [{status}] {time_part} {link} {rest_part} ^{bid}\n"
            else:
                return f"{indent_str}- [{status}] {link} {text} ^{bid}\n"
        else:
            clean_text = self.clean_task_text(text, bid, fname)
            creation_date = None
            if dates and re.match(r'^\d{4}-\d{2}-\d{2}$', str(dates).strip()):
                creation_date = str(dates).strip()
            if not creation_date:
                patterns = [
                    r'\[\[(\d{4}-\d{2}-\d{2})\]\]',
                    r'\[\[(\d{4}-\d{2}-\d{2})(?:#|\|)',
                    r'(?:ğŸ“…|\|ğŸ“…\]\])\s*(\d{4}-\d{2}-\d{2})'
                ]
                for p in patterns:
                    m = re.search(p, str(dates)) or re.search(p, text)
                    if m: creation_date = m.group(1); break
            if not creation_date:
                today = datetime.date.today().strftime('%Y-%m-%d')
                if dates:
                    Logger.info(f"âš ï¸ [FORMAT WARNING] æ—¥æœŸè§£æå¤±è´¥ï¼è¾“å…¥: '{dates}' -> å…œåº•: '{today}'")
                creation_date = today

            date_link = f"[[{creation_date}#^{bid}|â®]]"
            processed_dates = []
            done_date_match = re.search(r'âœ…\s*(\d{4}-\d{2}-\d{2})', str(dates))
            if done_date_match: processed_dates.append(f"âœ… {done_date_match.group(1)}")
            meta_str = " ".join(processed_dates)

            parts = [date_link]
            if clean_text: parts.append(clean_text)
            if meta_str: parts.append(meta_str)
            parts.append(f"^{bid}")

            return f"{indent_str}- [{status}] {' '.join(parts)}\n"

    # === [MODIFIED] Anchor-Based Normalization ===
    def normalize_child_lines(self, raw_lines, target_parent_indent, source_parent_indent=None, as_quoted=False):
        """
        [v14.0 Relative Anchor Normalization]
        ä½¿ç”¨ç›¸å¯¹åç§»é‡é‡æ„å­è¡Œï¼Œå®Œç¾ä¿ç•™å¤æ‚åˆ—è¡¨ç»“æ„ï¼ˆå›¾ç‰‡ã€å¼•ç”¨ã€å¤šçº§åˆ—è¡¨ï¼‰ã€‚

        Args:
            raw_lines: å­è¡Œåˆ—è¡¨ (ä¸åŒ…å«çˆ¶è¡Œ)
            target_parent_indent: çˆ¶è¡Œåœ¨ç›®æ ‡æ–‡ä»¶ä¸­çš„ç¼©è¿› (int, visual depth)
            source_parent_indent: çˆ¶è¡Œåœ¨æºæ–‡ä»¶ä¸­çš„åŸå§‹ç¼©è¿› (int, visual depth)
        """
        if not raw_lines: return []

        # å¦‚æœæœªæä¾›æºç¼©è¿›ï¼Œå°è¯•ä»ç¬¬ä¸€è¡Œåæ¨ï¼ˆå…œåº•ç­–ç•¥ï¼‰
        if source_parent_indent is None:
            if raw_lines:
                source_parent_indent = max(0, self._get_indent_depth(raw_lines[0]) - 4)
            else:
                source_parent_indent = 0

        children = []
        for line in raw_lines:
            content_cleaned = re.sub(r'^[>\s]+', '', line).strip()
            if not content_cleaned:
                children.append(("> \n" if as_quoted else "\n"))
                continue

            # 1. è®¡ç®—å½“å‰è¡Œç›¸å¯¹äºâ€œåŸçˆ¶çº§â€çš„åç§»é‡
            current_depth = self._get_indent_depth(line)
            delta = max(0, current_depth - source_parent_indent)

            # 2. è®¡ç®—ç›®æ ‡ç¼©è¿›
            target_depth = target_parent_indent + delta

            # 3. è½¬æ¢ä¸ºç¼©è¿›å­—ç¬¦ä¸² (ä½¿ç”¨ç©ºæ ¼æ›´å®‰å…¨ï¼Œæˆ–æŒ‰éœ€è½¬ Tab)
            # è¿™é‡Œç»Ÿä¸€ä½¿ç”¨ Space ç¡®ä¿å±‚çº§å‡†ç¡®ï¼Œåç»­ format_line è‹¥ç”¨ Tab å¯èƒ½éœ€è¦è½¬æ¢ï¼Œ
            # ä½†é€šå¸¸å­å†…å®¹å¯ä»¥ä¿æŒ Spaceã€‚å¦‚æœå¿…é¡» Tabï¼Œå¯ä»¥ç”¨ '\t' * (target_depth // 4)
            indent_str = ' ' * target_depth

            final = f"{indent_str}{content_cleaned}"
            if as_quoted: final = f"> {final}"
            children.append(final + "\n")

        return children

    # === [MODIFIED] Bridge with Anchor ===
    def reconstruct_daily_block(self, sd, target_date):
        fname = sd['fname']
        bid = sd['bid']
        status = sd['status']
        text = re.sub(r'\[\[\d{4}-\d{2}-\d{2}\]\]', '', sd['pure']).strip()
        link_tag = f"[[{fname}]]"
        if link_tag not in text: text = f"{link_tag} {text}"

        # ä¼ é€’ sd['indent'] ä½œä¸º source_parent_indent
        parent_line = self.format_line(sd['indent'], status, text, "", fname, bid, True)
        children = self.normalize_child_lines(
            sd['raw'][1:],
            target_parent_indent=sd['indent'],
            source_parent_indent=sd['indent'],
            as_quoted=False
        )
        return [parent_line] + children

    def ensure_structure(self, lines):
        has_dp = any(l.strip() == "# Day planner" for l in lines)
        j_idx = -1
        try:
            j_idx = next(i for i, l in enumerate(lines) if l.strip() == "# Journey")
        except StopIteration:
            pass
        if not has_dp:
            if j_idx != -1:
                lines.insert(j_idx, "# Day planner\n\n")
            else:
                lines.insert(0, "# Day planner\n\n");
                lines.append("\n# Journey\n")
        if has_dp and j_idx == -1: lines.append("\n# Journey\n")
        return lines

    def cleanup_empty_headers(self, lines, date_tag):
        lines = self.ensure_structure(lines)
        cleaned_lines = []
        i = 0
        modified = False
        current_section = None
        target_sections = ['# Day planner', '# Journey']
        while i < len(lines):
            line = lines[i]
            s_line = line.strip()
            if s_line.startswith('# '):
                current_section = s_line;
                cleaned_lines.append(line);
                i += 1;
                continue
            if current_section not in target_sections:
                cleaned_lines.append(line);
                i += 1;
                continue
            if s_line.startswith('## '):
                has_content = False
                j = i + 1
                while j < len(lines):
                    next_s = lines[j].strip()
                    if next_s.startswith('# ') or next_s.startswith('## ') or next_s == '----------': break
                    if next_s: has_content = True; break
                    j += 1
                if not has_content:
                    modified = True;
                    i = j
                else:
                    cleaned_lines.append(line);
                    i += 1
            else:
                cleaned_lines.append(line);
                i += 1
        return cleaned_lines, modified

    def scan_projects(self):
        self.project_map = {}
        self.project_path_map = {}
        self.file_path_map = {}

        # é¢„å¤„ç†ï¼šæ ‡å‡†åŒ–èšåˆç›®å½•è·¯å¾„ï¼Œé¿å…ä¸åŒç³»ç»Ÿçš„æ–œæ å·®å¼‚
        forced_dirs = [os.path.normpath(p) for p in Config.FORCED_AGGREGATION_DIRS]

        for root, dirs, files in os.walk(Config.ROOT_DIR):
            dirs[:] = [d for d in dirs if not FileUtils.is_excluded(os.path.join(root, d))]
            if FileUtils.is_excluded(root): continue

            main_files = []
            for f in files:
                if f.endswith('.md'):
                    path = os.path.join(root, f)
                    f_name = unicodedata.normalize('NFC', os.path.splitext(f)[0])
                    self.file_path_map[f_name] = path

                    # ä¾ç„¶è¯»å– tagsï¼Œä¿æŒæ–‡ä»¶çº§åˆ«çš„è¯†åˆ«èƒ½åŠ›
                    if 'main' in self.parse_yaml_tags(FileUtils.read_file(path) or []):
                        main_files.append(f)

            # === [æ ¸å¿ƒä¿®æ”¹ START] ===
            is_shadowed = False
            norm_root = os.path.normpath(root)

            for parent_dir in forced_dirs:
                if norm_root.startswith(parent_dir) and len(norm_root) > len(parent_dir):
                    rel_path = norm_root[len(parent_dir):]
                    if rel_path.startswith(os.sep):
                        is_shadowed = True
                        break

            if is_shadowed:
                continue
            # === [æ ¸å¿ƒä¿®æ”¹ END] ===

            if len(main_files) == 1:
                p_name = unicodedata.normalize('NFC', os.path.splitext(main_files[0])[0])
                self.project_map[root] = p_name
                self.project_path_map[p_name] = os.path.join(root, main_files[0])

    def scan_all_source_tasks(self) -> Dict[str, Dict]:
        self.scan_projects()
        source_data_by_date = {}
        today_str = datetime.date.today().strftime('%Y-%m-%d')
        for root, dirs, files in os.walk(Config.ROOT_DIR):
            dirs[:] = [d for d in dirs if not FileUtils.is_excluded(os.path.join(root, d))]
            if FileUtils.is_excluded(root): continue
            curr_proj = None
            temp = root
            while temp.startswith(Config.ROOT_DIR):
                if temp in self.project_map: curr_proj = self.project_map[temp]; break
                temp = os.path.dirname(temp)
                if temp == os.path.dirname(temp): break
            if not curr_proj: continue
            for f in files:
                if not f.endswith('.md'): continue
                path = os.path.join(root, f)
                lines = FileUtils.read_file(path)
                if not lines: continue
                mod = False
                fname = os.path.splitext(f)[0]
                i = 0

                in_task_section = False
                current_section_date = None
                seen_section_dates = set()
                while i < len(lines):
                    line = lines[i]
                    stripped = line.strip()
                    if stripped == '# Tasks':
                        in_task_section = True;
                        current_section_date = None;
                        seen_section_dates.clear();
                        i += 1;
                        continue
                    if stripped == '----------':
                        in_task_section = False;
                        current_section_date = None;
                        i += 1;
                        continue
                    if not in_task_section: i += 1; continue
                    header_match = re.match(r'^#+\s*\[\[\s*(\d{4}-\d{2}-\d{2})\s*\]\]', stripped)
                    if header_match:
                        date_str = header_match.group(1)
                        if date_str in seen_section_dates:
                            Logger.info(f"   ğŸ” å‘ç°é‡å¤æ ‡é¢˜ {date_str}ï¼Œå°†è§¦å‘é‡ç»„...");
                            mod = True
                        else:
                            seen_section_dates.add(date_str)
                        current_section_date = date_str;
                        i += 1;
                        continue
                    if stripped.startswith('#'): current_section_date = None; i += 1; continue
                    if not re.match(r'^\s*-\s*\[.\]', line): i += 1; continue
                    task_date = None
                    if current_section_date:
                        task_date = current_section_date
                    else:
                        date_match = re.search(r'[ğŸ“…âœ…]\s*(\d{4}-\d{2}-\d{2})', line)
                        if date_match:
                            task_date = date_match.group(1)
                        else:
                            link_match = re.search(r'\[\[(\d{4}-\d{2}-\d{2})(?:#|\||\]\])', line)
                            if link_match: task_date = link_match.group(1)
                    is_in_inbox_area = (current_section_date is None)
                    if is_in_inbox_area and not task_date: i += 1; continue
                    if not task_date: task_date = today_str; mod = True

                    # [MODIFIED] Use visual depth
                    indent = self._get_indent_depth(line)

                    status_match = re.search(r'-\s*\[(.)\]', line)
                    st = status_match.group(1) if status_match else ' '
                    id_m = re.search(r'\^([a-zA-Z0-9]{6,7})\s*$', line)
                    bid = id_m.group(1) if id_m else None
                    if not bid:
                        raw_block, _ = self.capture_block(lines, i)
                        temp_clean = self.clean_task_text(line, None, fname)
                        temp_clean = re.sub(r'\s+\^?[a-zA-Z0-9]*$', '', temp_clean).strip()
                        combined_body = self.normalize_block_content(raw_block[1:])
                        temp_combined_text = temp_clean + "|||" + combined_body
                        recovery_hash = self.sm.calc_hash(st, temp_combined_text)
                        found_id = self.sm.find_id_by_hash(path, recovery_hash)
                        if found_id:
                            Logger.info(f"   ğŸš‘ [RESCUE] æŒ‡çº¹åŒ¹é…æˆåŠŸ! '{temp_clean[:10]}...' -> å¤æ´» ID: {found_id}")
                            bid = found_id;
                            mod = True
                        else:
                            bid = self.generate_block_id().replace('^', '');
                            mod = True
                    clean_txt = self.clean_task_text(line, bid, context_name=fname)
                    dates_pattern = r'([ğŸ“…âœ…]\s*\d{4}-\d{2}-\d{2}|\[\[\d{4}-\d{2}-\d{2}(?:#\^[a-zA-Z0-9]+)?(?:\|[ğŸ“…â®])?\]\])'
                    dates = " ".join(re.findall(dates_pattern, line))
                    if current_section_date and current_section_date not in dates: dates = f"[[{task_date}]]"; mod = True
                    if task_date not in line and not dates: dates = f"[[{task_date}]]"; mod = True
                    new_line = self.format_line(indent, st, clean_txt, dates, fname, bid, False)
                    if new_line.strip() != line.strip(): lines[i] = new_line; mod = True

                    # [TIME GATE]
                    if task_date < Config.SYNC_START_DATE:
                        _, consumed = self.capture_block(lines, i)
                        i += consumed
                        continue

                    block, consumed = self.capture_block(lines, i)
                    combined_text = clean_txt + "|||" + self.normalize_block_content(block[1:])
                    content_hash = self.sm.calc_hash(st, combined_text)
                    if task_date not in source_data_by_date: source_data_by_date[task_date] = {}
                    source_data_by_date[task_date][bid] = {
                        'proj': curr_proj, 'bid': bid, 'pure': clean_txt, 'status': st,
                        'path': path, 'fname': fname, 'raw': block, 'hash': content_hash, 'indent': indent,
                        'dates': dates, 'is_quoted': False
                    }
                    i += consumed
                if mod:
                    lines = self.inject_into_task_section(lines, [])
                    # [CHECK] æ¯”å¯¹ç£ç›˜æ–‡ä»¶ï¼Œé˜²æ­¢æ­»å¾ªç¯
                    orig = FileUtils.read_file(path)
                    new_c = "".join(lines)
                    old_c = "".join(orig) if orig else ""
                    if new_c != old_c:
                        Logger.info(f"   ğŸ’¾ [WRITE] è‡ªåŠ¨æ ¼å¼åŒ–æºæ–‡ä»¶ (Scan): {os.path.basename(path)}")
                        FileUtils.write_file(path, lines)
        for delta in range(3):
            target_d = datetime.date.today() - datetime.timedelta(days=delta)
            target_s = target_d.strftime('%Y-%m-%d')
            if target_s not in source_data_by_date: source_data_by_date[target_s] = {}
        return source_data_by_date

    def organize_orphans(self, filepath, date_tag):
        lines = FileUtils.read_file(filepath)
        if not lines: return set()
        lines = self.ensure_structure(lines)
        tasks_to_move = []
        processed_bids = set()
        ctx = "ROOT"
        i = 0
        while i < len(lines):
            l = lines[i].strip()
            if l.startswith('# '):
                ctx = 'JOURNEY' if l == '# Journey' else ('PLANNER' if l == '# Day planner' else 'OTHER')
                i += 1;
                continue
            if l.startswith('## [[') and l.endswith(']]'): ctx = 'PROJECT'; i += 1; continue
            if re.match(r'^[\s>]*-\s*\[.\]', lines[i]):
                if ctx in ['JOURNEY', 'PLANNER']:
                    routing_target = self.extract_routing_target(lines[i])
                    p_name = None
                    if routing_target:
                        curr = os.path.dirname(routing_target)
                    else:
                        curr = Config.ROOT_DIR
                    search_start = curr
                    while search_start.startswith(Config.ROOT_DIR):
                        if search_start in self.project_map: p_name = self.project_map[search_start]; break
                        parent = os.path.dirname(search_start)
                        if parent == search_start: break
                        search_start = parent
                    if p_name:
                        content, length = self.capture_block(lines, i)
                        raw_first = content[0]
                        bid_m = re.search(r'\^([a-zA-Z0-9]{6,})\s*$', raw_first)
                        bid = bid_m.group(1) if bid_m else self.generate_block_id().replace('^', '')

                        final_tag_name = p_name
                        if routing_target: final_tag_name = os.path.splitext(os.path.basename(routing_target))[0]

                        # [MODIFIED] ä¿ç•™æ—¶é—´å¹¶ä¿®æ”¹é“¾æ¥æŒ‡å‘
                        clean_pure = self.clean_task_text(raw_first, bid, final_tag_name)

                        # Use raw string slicing for preservation, but logic relies on capture_block
                        indent_len = len(raw_first) - len(raw_first.lstrip())
                        indent_str = raw_first[:indent_len]

                        st_m = re.search(r'-\s*\[(.)\]', raw_first)
                        status = st_m.group(1) if st_m else ' '

                        # 1. æå–æ—¶é—´ (é˜²æ­¢ clean_task_text æŠŠæ—¶é—´åæ‰)
                        time_part = ""
                        body_only = re.sub(r'^\s*-\s*\[.\]\s?', '', raw_first)
                        tm = re.match(r'^(\d{1,2}:\d{2}(?:\s*-\s*\d{1,2}:\d{2})?)', body_only)
                        if tm: time_part = tm.group(1) + " "

                        # 2. é“¾æ¥æŒ‡å‘é¡¹ç›®æ–‡ä»¶ (Target) è€Œé æ—¥æœŸ (Date)
                        ret_link = f"[[{final_tag_name}#^{bid}|â®]]"

                        file_tag = f"[[{final_tag_name}]]"
                        # 3. ç»„è£…ï¼šç¼©è¿› - [x] æ—¶é—´ é“¾æ¥ æ–‡ä»¶æ ‡ç­¾ å†…å®¹ ID
                        new_line = f"{indent_str}- [{status}] {time_part}{ret_link} {file_tag} {clean_pure} ^{bid}\n"

                        content[0] = new_line
                        tasks_to_move.append({'idx': i, 'len': length, 'proj': p_name, 'raw': content})
                        processed_bids.add(bid)
                        i += length;
                        continue
            i += 1
        if not tasks_to_move: return set()
        tasks_to_move.sort(key=lambda x: x['idx'], reverse=True)
        for t in tasks_to_move: del lines[t['idx']:t['idx'] + t['len']]
        grouped = {}
        for t in tasks_to_move:
            if t['proj'] not in grouped: grouped[t['proj']] = []
            grouped[t['proj']].extend(t['raw'])
        try:
            j_idx = next(i for i, l in enumerate(lines) if l.strip() == "# Journey")
        except:
            j_idx = len(lines)
        ins_pt = len(lines)
        for i in range(j_idx + 1, len(lines)):
            if lines[i].startswith('# '): ins_pt = i; break
        offset = 0
        for proj, blocks in grouped.items():
            header = f"## [[{proj}]]"
            h_idx = -1
            for k in range(j_idx, ins_pt + offset):
                if lines[k].strip() == header: h_idx = k; break
            if blocks and not blocks[-1].endswith('\n'): blocks[-1] += '\n'
            if h_idx != -1:
                sub_ins = ins_pt + offset
                for k in range(h_idx + 1, ins_pt + offset):
                    if lines[k].startswith('#'): sub_ins = k; break
                lines[sub_ins:sub_ins] = blocks
                offset += len(blocks)
            else:
                chunk = [f"\n{header}\n"] + blocks
                lines[ins_pt + offset:ins_pt + offset] = chunk
                offset += len(chunk)
        Logger.info(f"å½’æ¡£ {len(tasks_to_move)} ä¸ªæµæµªä»»åŠ¡", date_tag)
        Logger.info(f"   ğŸ’¾ [WRITE] æ›´æ–°å½’æ¡£æ–‡ä»¶ (Orphans): {os.path.basename(filepath)}")
        if FileUtils.write_file(filepath, lines): return processed_bids
        return set()

    def process_date(self, target_date, src_tasks_for_date):
        today_str = datetime.date.today().strftime('%Y-%m-%d')
        daily_path = os.path.join(Config.DAILY_NOTE_DIR, f"{target_date}.md")

        # [NEW] æ¨¡ç‰ˆåˆå§‹åŒ–
        if not os.path.exists(daily_path) and src_tasks_for_date:
            if os.path.exists(Config.TEMPLATE_FILE):
                try:
                    tmpl_lines = FileUtils.read_file(Config.TEMPLATE_FILE)
                    if tmpl_lines:
                        Logger.info(f"   ğŸ“„ [TEMPLATE] æ£€æµ‹åˆ°æœªæ¥/ç¼ºå¤±æ—¥è®°ï¼Œæ­£åœ¨ä»æ¨¡ç‰ˆåˆ›å»º: {target_date}.md")
                        FileUtils.write_file(daily_path, tmpl_lines)
                        time.sleep(0.1)
                except Exception as e:
                    Logger.error_once(f"tmpl_fail_{target_date}", f"æ¨¡ç‰ˆåˆ›å»ºå¤±è´¥: {e}")
            else:
                Logger.info(f"   âš ï¸ æœªæ‰¾åˆ°æ¨¡ç‰ˆæ–‡ä»¶ ({Config.REL_TEMPLATE_FILE})ï¼Œåˆ›å»ºåŸºç¡€éª¨æ¶: {target_date}.md")
                base_scaffold = ["# Day planner\n", "\n", "# Journey\n", "\n"]
                FileUtils.write_file(daily_path, base_scaffold)

        organized_bids = set()
        if os.path.exists(daily_path): organized_bids = self.organize_orphans(daily_path, target_date)
        dn_tasks = {}
        new_dn_tasks = []
        dn_lines = []
        if os.path.exists(daily_path):
            dn_lines = FileUtils.read_file(daily_path) or []
            curr_ctx = None;
            current_section = None;
            i = 0
            while i < len(dn_lines):
                line = dn_lines[i]
                if line.startswith('# '): current_section = line.strip()
                h_m = re.match(r'^##\s*\[\[(.*?)\]\]', line.strip())
                if h_m: curr_ctx = h_m.group(1); i += 1; continue
                tm = re.match(r'^[\s>]*-\s*\[(.)\]', line)
                if tm:
                    is_allowed_section = False
                    if current_section in Config.DAILY_NOTE_SECTIONS: is_allowed_section = True
                    if not is_allowed_section: i += 1; continue
                    lm = re.search(r'\[\[(.*?)\#\^([a-zA-Z0-9]{6,})\|.*?\]\]', line)
                    if lm:
                        ctx_name = lm.group(1);
                        bid = lm.group(2)
                        raw, c = self.capture_block(dn_lines, i)
                        clean = self.clean_task_text(line, bid, context_name=ctx_name)
                        st = tm.group(1)
                        combined_text = clean + "|||" + self.normalize_block_content(raw[1:])
                        content_hash = self.sm.calc_hash(st, combined_text)

                        # [MODIFIED] Store indent for reconstruction
                        indent_val = self._get_indent_depth(line)
                        dn_tasks[bid] = {'pure': clean, 'status': st, 'idx': i, 'len': c, 'raw': raw,
                                         'hash': content_hash, 'proj': curr_ctx, 'indent': indent_val}
                        i += c;
                        continue
                    elif curr_ctx and curr_ctx in self.project_path_map:
                        if '^' not in line:
                            raw_indent = self._get_indent_depth(line)  # [MODIFIED]
                            raw, c = self.capture_block(dn_lines, i)
                            new_dn_tasks.append({'proj': curr_ctx, 'idx': i, 'len': c, 'raw': raw, 'st': tm.group(1),
                                                 'indent': raw_indent})
                            i += c;
                            continue
                        else:
                            link_match = re.search(r'\[\[(.*?)(?:#|\||\]\])', line)
                            if link_match:
                                pot = link_match.group(1).strip()
                                pot = unicodedata.normalize('NFC', pot)
                                target_file = None
                                if pot in self.project_path_map:
                                    target_file = self.project_path_map[pot]
                                elif pot in self.file_path_map:
                                    target_file = self.file_path_map[pot]
                                if target_file:
                                    raw_indent = self._get_indent_depth(line)  # [MODIFIED]
                                    raw, c = self.capture_block(dn_lines, i)
                                    new_dn_tasks.append(
                                        {'proj': self.project_map.get(os.path.dirname(target_file), pot), 'idx': i,
                                         'len': c, 'raw': raw, 'st': tm.group(1), 'indent': raw_indent})
                                    i += c;
                                    continue
                i += 1

        dn_mod = False
        if new_dn_tasks:
            Logger.info(f"   [NEW] å‘ç° {len(new_dn_tasks)} ä¸ªå¾…æ³¨å†Œä»»åŠ¡")
            for nt in reversed(new_dn_tasks):
                p_name = nt['proj'];
                txt = nt['raw'][0];
                clean = self.clean_task_text(txt)
                tgt = self.extract_routing_target(txt) or self.project_path_map.get(p_name)
                if not tgt: continue
                bid = self.generate_block_id().replace('^', '')
                fname = os.path.splitext(os.path.basename(tgt))[0]
                Logger.info(f"   â• æ³¨å†Œä»»åŠ¡ {bid}:")
                s_l = self.format_line(nt['indent'], nt['st'], clean, target_date, fname, bid, False)
                # [MODIFIED] Pass source_parent_indent
                s_blk = [s_l] + self.normalize_child_lines(nt['raw'][1:], nt['indent'],
                                                           source_parent_indent=nt['indent'], as_quoted=True)
                d_l = self.format_line(nt['indent'], nt['st'], clean, "", fname, bid, True)
                d_blk = [d_l] + self.normalize_child_lines(nt['raw'][1:], nt['indent'],
                                                           source_parent_indent=nt['indent'], as_quoted=False)

                dn_lines[nt['idx']:nt['idx'] + nt['len']] = d_blk
                dn_mod = True
                sl = FileUtils.read_file(tgt) or []
                sl = self.inject_into_task_section(sl, s_blk)
                # [FIX] æ˜¾å¼æ¯”å¯¹ï¼Œé˜²æ­¢ None å¯¼è‡´ä¸¢åŒ…
                orig_sl = FileUtils.read_file(tgt) or []
                if "".join(sl) != "".join(orig_sl):
                    # === ğŸ¯ ç¬¬ä¸€æ¬¡æ—¥å¿—ä¿®æ”¹ (New Task) ===
                    Logger.info(f"   ğŸ’¾ [WRITE] å†™å…¥æºæ–‡ä»¶ (New Task) (from {target_date}): {os.path.basename(tgt)}")
                    FileUtils.write_file(tgt, sl)
                self.trigger_delayed_verification(tgt)
                combined_text = clean + "|||" + self.normalize_block_content(nt['raw'][1:])
                h = self.sm.calc_hash(nt['st'], combined_text)
                self.sm.update_task(bid, h, tgt, target_date)
        if dn_mod:
            Logger.info(f"   ğŸ’¾ [WRITE] æ›´æ–°æ—¥è®°æ–‡ä»¶ (Sync Pre-Save): {os.path.basename(daily_path)}")
            FileUtils.write_file(daily_path, dn_lines)
            self.sm.save()

        src_tasks = src_tasks_for_date
        all_ids = set(src_tasks.keys()) | set(dn_tasks.keys())
        append_to_dn = {}
        src_updates = {}
        src_deletes = {}

        for bid in all_ids:
            in_s = bid in src_tasks;
            in_d = bid in dn_tasks
            last_hash = self.sm.get_task_hash(bid);
            last_date = self.sm.get_task_date(bid)
            if in_s:
                sd = src_tasks[bid]
                if in_d:
                    dd = dn_tasks[bid]
                    s_changed = (sd['hash'] != last_hash);
                    d_changed = (dd['hash'] != last_hash)
                    if s_changed and not d_changed:
                        Logger.info(f"   ğŸ”„ S->D åŒæ­¥ ({bid}):")
                        blk = self.reconstruct_daily_block(sd, target_date)
                        dn_lines[dd['idx']:dd['idx'] + dd['len']] = blk
                        dn_mod = True
                        self.sm.update_task(bid, sd['hash'], sd['path'], target_date)
                    elif d_changed and not s_changed:
                        Logger.info(f"   ğŸ”„ D->S åŒæ­¥ ({bid}):")
                        n_l = self.format_line(sd['indent'], dd['status'], dd['pure'], target_date, sd['fname'], bid,
                                               False)
                        # [MODIFIED] Pass source_parent_indent (using daily indent)
                        blk = [n_l] + self.normalize_child_lines(dd['raw'][1:], sd['indent'],
                                                                 source_parent_indent=dd['indent'], as_quoted=False)
                        if sd['path'] not in src_updates: src_updates[sd['path']] = {}
                        src_updates[sd['path']][bid] = blk
                        self.sm.update_task(bid, dd['hash'], sd['path'], target_date)
                    elif s_changed and d_changed:
                        if sd['hash'] != dd['hash']:
                            Logger.info(f"   âš”ï¸ å†²çª ({bid}): Daily è¦†ç›– Source")
                            n_l = self.format_line(sd['indent'], dd['status'], dd['pure'], target_date, sd['fname'],
                                                   bid, False)
                            # [MODIFIED] Conflict resolution using Daily structure
                            blk = [n_l] + self.normalize_child_lines(dd['raw'][1:], sd['indent'],
                                                                     source_parent_indent=dd['indent'], as_quoted=False)
                            if sd['path'] not in src_updates: src_updates[sd['path']] = {}
                            src_updates[sd['path']][bid] = blk
                            self.sm.update_task(bid, dd['hash'], sd['path'], target_date)

                        else:
                            # [Fixed] çŠ¶æ€ç¨³å®šæ—¶ä»…æ›´æ–°å¿ƒè·³ï¼Œä¸è§¦å‘æ–‡ä»¶å†™å…¥
                            # if sd['path'] not in src_updates: src_updates[sd['path']] = {}
                            # src_updates[sd['path']][bid] = sd['raw']
                            self.sm.update_task(bid, sd['hash'], sd['path'], target_date)
                else:
                    if last_date == target_date:
                        Logger.info(f"   ğŸ—‘ï¸ åˆ é™¤ Source ({bid}): å›  Daily ç§»é™¤")
                        if sd['path'] not in src_deletes: src_deletes[sd['path']] = {}
                        src_deletes[sd['path']][bid] = sd['path']
                        self.sm.remove_task(bid)
                    else:
                        task_dates_str = sd.get('dates', '')
                        linked_dates = re.findall(r'(\d{4}-\d{2}-\d{2})', task_dates_str)
                        is_misjudged = False
                        if linked_dates and target_date not in linked_dates: is_misjudged = True
                        if is_misjudged:
                            Logger.info(f"   ğŸ›¡ï¸ æ‹¦æˆªè¿½åŠ  ({bid}): å½’å± {linked_dates} != å½“å‰ {target_date}")
                            continue
                        Logger.info(f"   â• è¿½åŠ  Daily ({bid}): æ¥è‡ª {sd['fname']}")
                        if sd['proj'] not in append_to_dn: append_to_dn[sd['proj']] = []
                        append_to_dn[sd['proj']].append(sd)
                        self.sm.update_task(bid, sd['hash'], sd['path'], target_date)

            elif in_d and not in_s:
                dd = dn_tasks[bid];
                raw_first = dd['raw'][0]
                db_data = self.sm.state.get(bid, {})
                last_path = db_data.get('source_path', '')
                is_daily_native = (not last_path) or (Config.DAILY_NOTE_DIR in last_path)
                target_file_direct = self.extract_routing_target(raw_first)
                is_deleted_from_source = False
                if target_file_direct and last_path:
                    p1 = os.path.normcase(os.path.abspath(target_file_direct))
                    p2 = os.path.normcase(os.path.abspath(last_path))
                    if p1 == p2: is_deleted_from_source = True
                should_push = (bid in organized_bids) or is_daily_native or (
                        target_file_direct and os.path.exists(target_file_direct) and not is_deleted_from_source)
                if should_push:
                    target_file = None
                    if target_file_direct:
                        target_file = target_file_direct
                    else:
                        p_name = dd.get('proj')
                        target_file = self.project_path_map.get(p_name)
                    if target_file and os.path.exists(target_file):
                        Logger.info(f"   ğŸš€ [GRADUATE] å½’æ¡£ä»»åŠ¡æ™‹å‡ä¸Šè¡Œ ({bid}) -> {os.path.basename(target_file)}")
                        fname = os.path.splitext(os.path.basename(target_file))[0]
                        clean = dd['pure']
                        raw_no_quote = re.sub(r'^>\s?', '', raw_first)

                        # [MODIFIED] Use visual depth
                        raw_indent = self._get_indent_depth(raw_no_quote)

                        n_l = self.format_line(raw_indent, dd['status'], clean, target_date, fname, bid, False)

                        # [MODIFIED] Pass source_parent_indent using dd['indent']
                        blk = [n_l] + self.normalize_child_lines(dd['raw'][1:], raw_indent,
                                                                 source_parent_indent=dd['indent'], as_quoted=False)

                        if target_file not in src_updates: src_updates[target_file] = {}
                        src_updates[target_file][bid] = blk
                        self.sm.update_task(bid, dd['hash'], target_file, target_date)
                    else:
                        Logger.info(f"   âš ï¸ [ORPHAN] æ— æ³•åŒæ­¥ï¼Œæ‰¾ä¸åˆ°ç›®æ ‡æ–‡ä»¶")
                else:
                    Logger.info(f"   ğŸ—‘ï¸ åˆ é™¤ Daily ({bid}): å›  Source ç§»é™¤")
                    for k in range(dd['idx'], dd['idx'] + dd['len']): dn_lines[k] = None
                    dn_mod = True

        if dn_mod:
            # [CRITICAL FIX] å†™å…¥æ—¥è®°æ–‡ä»¶å‰çš„å¹‚ç­‰æ€§æ£€æŸ¥
            final_dn_lines = [l for l in dn_lines if l is not None]
            original_dn_content = FileUtils.read_content(daily_path) or ""
            new_dn_content = "".join(final_dn_lines)

            if original_dn_content != new_dn_content:
                FileUtils.write_file(daily_path, final_dn_lines)
                Logger.info(f"   âœ… æ—¥è®°æ–‡ä»¶å·²å›å†™: {os.path.basename(daily_path)}")

        if src_deletes:
            for path, bids in src_deletes.items():
                sl = FileUtils.read_file(path)
                if not sl: continue
                out, i, chg = [], 0, False
                deleted_bids = list(bids.keys())
                while i < len(sl):
                    im = re.search(r'\^([a-zA-Z0-9]{6,})\s*$', sl[i])
                    if not im: im = re.search(r'\(connect::.*?\^([a-zA-Z0-9]{6,})\)', sl[i])
                    if im and im.group(1) in deleted_bids:
                        _, c = self.capture_block(sl, i);
                        i += c;
                        chg = True
                    else:
                        out.append(sl[i]);
                        i += 1
                if chg:
                    stem = os.path.splitext(os.path.basename(path))[0]
                    out = self.inject_into_task_section(out, [], stem)

                    # [FIX] æ˜¾å¼æ¯”å¯¹
                    orig_content = "".join(sl)
                    new_content = "".join(out)
                    if orig_content != new_content:
                        # === ğŸ¯ ç¬¬äºŒæ¬¡æ—¥å¿—ä¿®æ”¹ (Delete) ===
                        Logger.info(f"   ğŸ’¾ [WRITE] å†™å…¥æºæ–‡ä»¶ (Delete) (from {target_date}): {os.path.basename(path)}")
                        FileUtils.write_file(path, out)

        if src_updates:
            for path, ups in src_updates.items():
                sl = FileUtils.read_file(path)
                if not sl: sl = []
                out, i, chg = [], 0, False
                handled_bids = set()
                while i < len(sl):
                    im = re.search(r'\^([a-zA-Z0-9]{6,})\s*$', sl[i])
                    if not im: im = re.search(r'\(connect::.*?\^([a-zA-Z0-9]{6,})\)', sl[i])
                    if im and im.group(1) in ups:
                        bid = im.group(1)
                        _, c = self.capture_block(sl, i)
                        out.extend(ups[bid])
                        handled_bids.add(bid)
                        i += c;
                        chg = True
                    else:
                        out.append(sl[i]);
                        i += 1
                pending_inserts = []
                for bid, blk in ups.items():
                    if bid not in handled_bids: pending_inserts.extend(blk); chg = True

                if chg:
                    stem = os.path.splitext(os.path.basename(path))[0]
                    out = self.inject_into_task_section(out, pending_inserts, stem)

                    # [FIX] æ˜¾å¼æ¯”å¯¹ï¼Œé˜²æ­¢æ­»å¾ªç¯
                    orig_content = "".join(sl)
                    new_content = "".join(out)

                    if orig_content != new_content:
                        # === ğŸ¯ ç¬¬ä¸‰æ¬¡æ—¥å¿—ä¿®æ”¹ (Update/Insert) - ä½ çš„ä¸»è¦éœ€æ±‚ ===
                        Logger.info(
                            f"   ğŸ’¾ [WRITE] å†™å…¥æºæ–‡ä»¶ (Update/Insert) (from {target_date}): {os.path.basename(path)}")
                        FileUtils.write_file(path, out)
                        self.trigger_delayed_verification(path)

        self.sm.save()